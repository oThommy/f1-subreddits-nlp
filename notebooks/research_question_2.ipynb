{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pandas.io.formats.style import Styler\n",
    "from collections.abc import Generator, Callable\n",
    "import typing\n",
    "from typing import Any, TypeAlias\n",
    "import numpy as np\n",
    "from contextlib import contextmanager\n",
    "from functools import partial, reduce\n",
    "import re\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from IPython.display import (\n",
    "    display, # type: ignore[reportUnknownVariableType]\n",
    "    Markdown,\n",
    ")\n",
    "import importlib\n",
    "import spacy\n",
    "\n",
    "from config.fastf1 import fastf1\n",
    "import fastf1.events as fastf1_events\n",
    "from config import config\n",
    "importlib.reload(config);\n",
    "from src.data.loader import stream_ndjson, load_submissions_df, load_comments_df\n",
    "import src.data.preprocessing as preprocessing\n",
    "importlib.reload(preprocessing);\n",
    "import src.data.constants as dataset_constants\n",
    "import src.utils\n",
    "importlib.reload(src.utils);\n",
    "from src.utils import (\n",
    "    temporary_pandas_options,\n",
    "    display_full_dataframe,\n",
    "    hide_index,\n",
    "    compose,\n",
    ")\n",
    "from src import utils\n",
    "utils.set_random_seeds()\n",
    "\n",
    "import logging\n",
    "logging.getLogger('fastf1').setLevel(logging.WARNING)\n",
    "\n",
    "DEVICE = utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_f1_df(limit: int | None = None, in_place: bool = True) -> pd.DataFrame:\n",
    "    ndjson_streamer = partial(stream_ndjson, limit=limit)\n",
    "\n",
    "    return preprocessing.concatenate_submissions_and_comments(\n",
    "        submissions_df=load_submissions_df(dataset_constants.RawFile.FORMULA1_SUBMISSIONS, ndjson_streamer),\n",
    "        comments_df=load_comments_df(dataset_constants.RawFile.FORMULA1_COMMENTS, ndjson_streamer),\n",
    "        in_place=in_place,\n",
    "    )\n",
    "\n",
    "def load_f15_df(limit: int | None = None, in_place: bool = True) -> pd.DataFrame:\n",
    "    ndjson_streamer = partial(stream_ndjson, limit=limit)\n",
    "\n",
    "    return preprocessing.concatenate_submissions_and_comments(\n",
    "        submissions_df=load_submissions_df(dataset_constants.RawFile.FORMULA1POINT5_SUBMISSIONS, ndjson_streamer),\n",
    "        comments_df=load_comments_df(dataset_constants.RawFile.FORMULA1POINT5_COMMENTS, ndjson_streamer),\n",
    "        in_place=in_place,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_ndjson_streamer = partial(stream_ndjson, limit=100)\n",
    "f15_ndjson_streamer = partial(stream_ndjson, limit=100)\n",
    "\n",
    "f1_submissions_df = load_submissions_df(dataset_constants.RawFile.FORMULA1_SUBMISSIONS, f1_ndjson_streamer)\n",
    "f1_comments_df = load_comments_df(dataset_constants.RawFile.FORMULA1_COMMENTS, f1_ndjson_streamer)\n",
    "\n",
    "f15_submissions_df = load_submissions_df(dataset_constants.RawFile.FORMULA1POINT5_SUBMISSIONS, f15_ndjson_streamer)\n",
    "f15_comments_df = load_comments_df(dataset_constants.RawFile.FORMULA1POINT5_COMMENTS, f15_ndjson_streamer)\n",
    "\n",
    "f1_df = preprocessing.concatenate_submissions_and_comments(f1_submissions_df, f1_comments_df)\n",
    "f15_df = preprocessing.concatenate_submissions_and_comments(f15_submissions_df, f15_comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(Markdown('### r/formula1 submissions:'), f1_submissions_df.head(n))\n",
    "    display(Markdown('### r/formula1 comments:'), f1_comments_df.head(n))\n",
    "    display(Markdown('### r/formula1point5 submissions:'), f15_submissions_df.head(n))\n",
    "    display(Markdown('### r/formula1point5 comments:'), f15_comments_df.head(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(Markdown('### r/formula1 posts:'), f1_df.head(n))\n",
    "    display(Markdown('### r/formula1point5 posts:'), f15_df.head(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Rule-Based Prediction Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fastf1 historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_schedule = fastf1.get_event_schedule(dataset_constants.YEAR)\n",
    "schedule = typing.cast(\n",
    "    fastf1_events.EventSchedule,\n",
    "    full_schedule[\n",
    "        (full_schedule['EventDate'] >= dataset_constants.START_DATE) &\n",
    "        (full_schedule['EventDate'] <= dataset_constants.END_DATE) &\n",
    "        (full_schedule['EventFormat'] == 'conventional') # TODO: Skip sprint weekends for now. Also include sprint weekends later\n",
    "    ],\n",
    ")\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(schedule.iloc[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = f1_df\n",
    "race_weekend = schedule.iloc[-1]\n",
    "first_post_at = typing.cast(dt.datetime, race_weekend['Session1DateUtc']) - dt.timedelta(days=1)\n",
    "last_post_at = typing.cast(dt.datetime, race_weekend['Session5DateUtc'])\n",
    "posts_df = posts_df[\n",
    "    (posts_df['created_utc'] >= first_post_at) &\n",
    "    (posts_df['created_utc'] <= last_post_at)\n",
    "]\n",
    "\n",
    "def get_top20(race_weekend: fastf1_events.Event) -> pd.DataFrame:\n",
    "    race_session = race_weekend.get_session('Race')\n",
    "    race_session.load(laps=False, telemetry=False, weather=False, messages=False)\n",
    "    top20 = race_session.results[['FullName', 'Position']].astype({'Position': np.uint8})\n",
    "    return top20\n",
    "\n",
    "top20s = tuple(\n",
    "    get_top20(typing.cast(fastf1_events.Event, race_weekend))\n",
    "    for _, race_weekend in schedule.iterrows()\n",
    ")\n",
    "display(hide_index(top20s[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from scipy.special import softmax\n",
    "\n",
    "def driver_sentiment(comments, driver_list, scores):\n",
    "    model_name = \"yangheng/deberta-v3-base-absa-v1.1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    results = {driver: {\"positive\": 0.0, \"neutral\": 0.0, \"negative\": 0.0, \"count\": 0} for driver in driver_list}\n",
    "\n",
    "    for comment, score in zip(comments, scores):\n",
    "        found_drivers = [driver for driver in driver_list if driver in comment]\n",
    "        \n",
    "        for aspect in found_drivers:\n",
    "            inputs = tokenizer(comment, aspect, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            sent = outputs.logits[0].cpu().numpy()\n",
    "            probabilities = softmax(sent)\n",
    "\n",
    "            results[aspect][\"positive\"] += probabilities[2] * score\n",
    "            results[aspect][\"neutral\"] += probabilities[1] * score\n",
    "            results[aspect][\"negative\"] += probabilities[0] * score\n",
    "            results[aspect][\"count\"] += score\n",
    "\n",
    "    for driver, sentiment in results.items():\n",
    "        if sentiment[\"count\"] > 0:\n",
    "            sentiment[\"positive\"] /= sentiment[\"count\"]\n",
    "            sentiment[\"neutral\"] /= sentiment[\"count\"]\n",
    "            sentiment[\"negative\"] /= sentiment[\"count\"]\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "prediction_posts_df = ['Carlos Sainz is loving this upgraded car, good top 3 for the race tomorrow! I disagree with you, Max Verstappen will definitely finish first. I think BOT will finish behind NOR, who will probably finish 7th. That\\'s my opinion at least... I predict that the RedBulls with finish 1-2. Nah, the Danish driver from Haas will almost certainly finish in points! Stroll on the podium and Vettel in points. I like cookies!']\n",
    "driver_list = ['Carlos Sainz', 'Max Verstappen']\n",
    "\n",
    "F1_names= {\n",
    "    'max verstappen',\n",
    "    'charles leclerc',\n",
    "    'sergio perez',\n",
    "    'george russell',\n",
    "    'carlos sainz',\n",
    "    'lewis hamilton',\n",
    "    'lando norris',\n",
    "    'esteban ocon',\n",
    "    'fernando alonso',\n",
    "    'valtteri bottas',\n",
    "    'daniel ricciardo',\n",
    "    'sebastian vettel',\n",
    "    'kevin magnussen',\n",
    "    'pierre gasly',\n",
    "    'lance stroll',\n",
    "    'mick schumacher',\n",
    "    'yuki tsunoda',\n",
    "    'zhou guanyu',\n",
    "    'alexander albon',\n",
    "    'nicholas latifi',\n",
    "    'nyck de vries',\n",
    "    'nico hulkenberg',\n",
    "    'oscar piastri',\n",
    "    'liam lawson',\n",
    "    'logan sargeant'\n",
    "}\n",
    "\n",
    "posts_df = load_f1_df(1000)\n",
    "posts_df['text'] = posts_df['text'].apply(preprocessing.correct_spelling_in_text_spacy)\n",
    "comments = posts_df[\"text\"].tolist()\n",
    "scores = posts_df[\"score\"].tolist()\n",
    "\n",
    "\n",
    "results = driver_sentiment(comments, F1_names, scores)\n",
    "print(results)\n",
    "for driver, sentiment in results.items():\n",
    "    if sentiment[\"count\"] > 0:\n",
    "        print(f\"{driver}: [Positive: {sentiment['positive']:.4f}, Neutral: {sentiment['neutral']:.4f}, Negative: {sentiment['negative']:.4f}]\")\n",
    "    else:\n",
    "        print(f\"{driver}: No mentions found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_scores(results):\n",
    "    final_scores = []\n",
    "\n",
    "    for driver, sentiment in results.items():\n",
    "        if sentiment[\"count\"] > 0:\n",
    "            sentiment_score = (sentiment[\"positive\"] - sentiment[\"negative\"])\n",
    "            final_scores.append((driver.title(), sentiment_score))\n",
    "\n",
    "    # Sort drivers by positive - negative score (descending order)\n",
    "    final_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return final_scores\n",
    "\n",
    "# Print sorted results\n",
    "scores = final_scores(results)\n",
    "\n",
    "print(\"Drivers ranked by (positive - negative score):\")\n",
    "for driver, score in scores:\n",
    "    print(f\"{driver}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(n_event, final_scores, n_events=5, historical_score_contribution=0.4):\n",
    "    #func to import historical data for this race\n",
    "    historical_data_event = get_top20(full_schedule.iloc[n_event])\n",
    "    historical_scores = {row[\"FullName\"]: 0 for _, row in historical_data_event.iterrows()}\n",
    "\n",
    "    for i in range(n_events):\n",
    "        #func to import historical data for one of the last 5 races\n",
    "        historical_data = get_top20(full_schedule.iloc[n_event - (i+1)])\n",
    "\n",
    "        for _, row in historical_data.iterrows():\n",
    "            historical_scores[row[\"FullName\"]] += 1 - ((row[\"Position\"] - 1) / 19) * 2\n",
    "\n",
    "    for driver, score in historical_scores.items():\n",
    "        historical_scores[driver] = score / n_events\n",
    "\n",
    "    final_scores_dict = dict(final_scores)\n",
    "\n",
    "    final_prediction = []\n",
    "    for driver, historical_score in historical_scores.items():\n",
    "        if driver in final_scores_dict:\n",
    "            score = final_scores_dict[driver]\n",
    "            \n",
    "            combined_score = (1 - historical_score_contribution) * score + historical_score_contribution * historical_score\n",
    "            final_prediction.append((driver, combined_score, score, historical_score))\n",
    "        else:\n",
    "            final_prediction.append((driver, historical_score, None, historical_score))\n",
    "\n",
    "    final_prediction.sort(key=lambda x: x[1], reverse=True)\n",
    "    final_prediction_dict = {\n",
    "        driver: {\"combined_score\": comb_score, \"sentiment_score\": pred_score, \"historical_score\": hist_score}\n",
    "        for driver, comb_score, pred_score, hist_score in final_prediction\n",
    "    }\n",
    "    # Calculate MAE\n",
    "    # Map drivers to their predicted positions\n",
    "    predicted_positions = {driver: i + 1 for i, (driver, _, _, _) in enumerate(final_prediction)}\n",
    "\n",
    "    # Map drivers to their true positions\n",
    "    true_positions = {row[\"FullName\"]: row[\"Position\"] for _, row in historical_data_event.iterrows()}\n",
    "\n",
    "    # Compute absolute errors for drivers present in both sets\n",
    "    data = []\n",
    "    for driver, predicted_position in predicted_positions.items():\n",
    "        true_position = true_positions.get(driver, None)\n",
    "        scores = final_prediction_dict.get(driver, {\"combined_score\": None, \"sentiment_score\": None, \"historical_score\": None})\n",
    "\n",
    "        error = abs(predicted_position - true_position) if true_position is not None else None\n",
    "        data.append({\n",
    "            \"driver_name\": driver,\n",
    "            \"predicted_position\": predicted_position,\n",
    "            \"true_position\": true_position,\n",
    "            \"error\": error,\n",
    "            \"combined_score\": scores[\"combined_score\"],\n",
    "            \"sentiment_score\": scores[\"sentiment_score\"],\n",
    "            \"historical_score\": scores[\"historical_score\"]\n",
    "        })\n",
    "\n",
    "    prediction_df = pd.DataFrame(data)\n",
    "\n",
    "    return prediction_df\n",
    "\n",
    "index = 9\n",
    "prediction_df = prediction(index, scores, n_events=5, historical_score_contribution=0.4)\n",
    "display(prediction_df)\n",
    "mae = prediction_df[\"error\"].mean()\n",
    "print(mae)\n",
    "position = 0\n",
    "# for driver, score in final_prediction:\n",
    "#     position += 1\n",
    "#     print(f\"{driver} finishes in position:{position}      {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLiNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"C:\\\\cache\"\n",
    "from gliner import GLiNER\n",
    "\n",
    "gliner_pickle_path = config.DATA_DIR / '.cache' / 'gliner_model.pkl'\n",
    "gliner_pickle_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "use_cache = False\n",
    "\n",
    "if use_cache:\n",
    "    if not gliner_pickle_path.exists():\n",
    "        gliner_model = GLiNER.from_pretrained('urchade/gliner_medium-v2.1')\n",
    "\n",
    "        with open(gliner_pickle_path, 'wb') as file:\n",
    "            pickle.dump(gliner_model, file)\n",
    "    else:\n",
    "        with open(gliner_pickle_path, 'rb') as file:\n",
    "            gliner_model = pickle.load(file)\n",
    "else:\n",
    "    gliner_model = GLiNER.from_pretrained('urchade/gliner_medium-v2.1')\n",
    "\n",
    "gliner_model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(gliner_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = 'Carlos Sainz is loving this upgraded car, good top 3 for the race tomorrow! I disagree with you, verstappening will definitely finish first. I think BOT will finish behind NOR, who will probably finish 7th. That\\'s my opinion at least... I predict that the RedBulls with finish 1-2. Nah, the Danish driver from Haas will almost certainly finish in points! Stroll on the podium and Vettel in points. I like cookies!'\n",
    "debug = True\n",
    "\n",
    "if debug:\n",
    "    doc = nlp(text)\n",
    "    df = pd.DataFrame({'text': tuple(sentence.text for sentence in doc.sents)})\n",
    "else:\n",
    "    df = load_f1_df(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with display_full_dataframe():\n",
    "    display(df.head())\n",
    "\n",
    "def has_prediction(post_text: str, threshold: float = 0.45) -> bool:\n",
    "    # doc = nlp(post_text)\n",
    "\n",
    "    # TODO: does GLiNER's performance improve with more context? if yes, refactor to chunking instead of going over each sentence individually\n",
    "    # for sentence in doc.sents:\n",
    "    # TODO: for some reason, if you include only 'position', the predictions are far worse than with 'driver' included\n",
    "    with torch.no_grad():\n",
    "        entities = gliner_model.predict_entities(post_text, ('driver', 'position',), threshold=threshold) # TODO: very low threshold\n",
    "    position_entities = tuple(entity for entity in entities if entity['label'] == 'position')\n",
    "\n",
    "    if debug:\n",
    "        print(position_entities)\n",
    "        print(tuple(position['text'] for position in position_entities))\n",
    "\n",
    "    if len(position_entities) != 0:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "predictions_df = df[df['text'].apply(has_prediction)]\n",
    "print(len(predictions_df))\n",
    "with display_full_dataframe():\n",
    "    display(predictions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def has_prediction_dask(post_text):\n",
    "#     return has_prediction(post_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.dataframe as dd\n",
    "\n",
    "# # Convert the Pandas DataFrame to a Dask DataFrame\n",
    "# dask_df = dd.from_pandas(load_f1_df(10), npartitions=16)  # Adjust the number of partitions as needed\n",
    "\n",
    "# # Apply the function in parallel\n",
    "# dask_df['has_prediction'] = dask_df['text'].map(has_prediction_dask, meta=('text', 'bool'))\n",
    "\n",
    "# # Compute the result and convert back to a Pandas DataFrame\n",
    "# result_df = dask_df[dask_df['has_prediction']].compute()\n",
    "\n",
    "# # Display the filtered DataFrame\n",
    "# with display_full_dataframe():\n",
    "#     display(result_df.head())\n",
    "historical_data_event = get_top20(full_schedule.iloc[index])\n",
    "true_positions = {row[\"FullName\"]: row[\"Position\"] for _, row in historical_data_event.iterrows()}\n",
    "display(true_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Carlos Sainz is loving this upgraded car, good top 3 for the race tomorrow! I disagree with you, verstappening will definitely finish first. I think BOT will finish behind NOR, who will probably finish 7th. That\\'s my opinion at least... I predict that the RedBulls with finish 1-2. Nah, the Danish driver from Haas will almost certainly finish in points! Stroll on the podium and Vettel in points. I like cookies!'\n",
    "debug = False\n",
    "import dask.dataframe as dd\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"Sentence of length .* has been truncated to .*\",\n",
    "    category=UserWarning\n",
    ")\n",
    "\n",
    "if debug:\n",
    "    doc = nlp(text)\n",
    "    posts_df = pd.DataFrame({'text': tuple(sentence.text for sentence in doc.sents)})\n",
    "else:\n",
    "    start = time.perf_counter_ns()\n",
    "    all_posts_df = load_f1_df()\n",
    "    end = time.perf_counter_ns()\n",
    "    print((end - start) / 10 ** 9, \"load time\")\n",
    "\n",
    "# with display_full_dataframe():\n",
    "#     display(hide_index(df.head()))\n",
    "\n",
    "# df['text'] = df['text'].apply(preprocessing.correct_spelling_in_text_spacy)\n",
    "# df = df[df['text'].apply(has_prediction)]\n",
    "\n",
    "def display_posts_df(n=3):\n",
    "    global posts_df\n",
    "    \n",
    "    with display_full_dataframe():\n",
    "        display(hide_index(posts_df.head(n)))\n",
    "\n",
    "predictions_dict = {}\n",
    "mae_list = []\n",
    "\n",
    "for index, race_weekend in schedule.iterrows():\n",
    "    print(index, race_weekend)\n",
    "    #load relevant post\n",
    "    first_post_at = typing.cast(dt.datetime, race_weekend['Session4DateUtc']) #- dt.timedelta(days=1)\n",
    "    last_post_at = typing.cast(dt.datetime, race_weekend['Session5DateUtc'])\n",
    "    posts_df = all_posts_df[\n",
    "        (all_posts_df['created_utc'] >= first_post_at) &\n",
    "        (all_posts_df['created_utc'] <= last_post_at)\n",
    "    ]\n",
    "    print('number of posts for event: ', len(posts_df))\n",
    "\n",
    "    # spelling correction\n",
    "    start = time.perf_counter_ns()\n",
    "    posts_df['text'] = posts_df['text'].apply(preprocessing.correct_spelling_in_text_spacy)\n",
    "    end = time.perf_counter_ns()\n",
    "    print((end - start) / 10 ** 9, \"spell time\")\n",
    "\n",
    "    # only predictions\n",
    "    start = time.perf_counter_ns()\n",
    "\n",
    "    # posts_ddf = dd.from_pandas(posts_df, npartitions=16)\n",
    "    # display(posts_ddf.head())\n",
    "    # has_prediction = posts_ddf['text'].map_partitions(has_prediction, meta=('text', 'bool'))\n",
    "    # print(type(has_prediction))\n",
    "    # print(has_prediction)\n",
    "    # print(has_prediction.compute())\n",
    "    # posts_df = posts_ddf[has_prediction].compute()\n",
    "    \n",
    "    posts_df = posts_df[posts_df['text'].apply(has_prediction)]\n",
    "    print('number of posts with prediction: ', len(posts_df))\n",
    "    # with display_full_dataframe():\n",
    "    #     display(predictions_df.head())\n",
    "\n",
    "    end = time.perf_counter_ns()\n",
    "    print((end - start) / 10 ** 9, \"filter time\")\n",
    "\n",
    "    # sentiment score\n",
    "    start = time.perf_counter_ns()\n",
    "    comments = posts_df[\"text\"].tolist()\n",
    "    upvotes = posts_df[\"score\"].tolist()\n",
    "    sentiment = driver_sentiment(comments, F1_names, upvotes)\n",
    "    scores = final_scores(sentiment)\n",
    "    end = time.perf_counter_ns()\n",
    "    print((end - start) / 10 ** 9, \"sentiment time\")\n",
    "\n",
    "    # final prediction\n",
    "    start = time.perf_counter_ns()\n",
    "    prediction_df = prediction(index, scores, n_events=5, historical_score_contribution=0.4)\n",
    "    mae = prediction_df[\"error\"].mean()\n",
    "    display(prediction_df)\n",
    "    print(\"the MAE for predicted vs true position is: \", mae)\n",
    "    end = time.perf_counter_ns()\n",
    "    print((end - start) / 10 ** 9, \"final pred time\")\n",
    "\n",
    "    predictions_dict[index] = prediction_df\n",
    "    print(\"\\n\" * 3)\n",
    "    \n",
    "\n",
    "with pd.ExcelWriter(config.DATA_DIR /final/\"predictions.xlsx\") as writer:\n",
    "    for key, df in predictions_dict.items():\n",
    "        df.to_excel(writer, sheet_name=f\"Iteration_{key}\", index=False)\n",
    "\n",
    "# loaded_predictions = pd.read_excel(config.DATA_DIR /final/\"predictions.xlsx\", sheet_name=None)  # Returns a dictionary of DataFrames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Drivers ranked by (positive - negative score):\")\n",
    "for driver, score in scores:\n",
    "    print(f\"{driver}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment score\n",
    "start = time.perf_counter_ns()\n",
    "comments = posts_df[\"text\"].tolist()\n",
    "upvotes = posts_df[\"score\"].tolist()\n",
    "sentiment = driver_sentiment(comments, F1_names, upvotes)\n",
    "scores = final_scores(sentiment)\n",
    "end = time.perf_counter_ns()\n",
    "print((end - start) / 10 ** 9, \"sentiment time\")\n",
    "# print(\"Drivers ranked by (positive - negative score):\")\n",
    "# for driver, score in scores:\n",
    "#     print(f\"{driver}: {score:.4f}\")\n",
    "\n",
    "# final prediction\n",
    "start = time.perf_counter_ns()\n",
    "pred = prediction(index, scores, n_events=5, historical_score_contribution=0.75)\n",
    "end = time.perf_counter_ns()\n",
    "print((end - start) / 10 ** 9, \"final pred time\")\n",
    "\n",
    "\n",
    "predictions_dict[index] = pred\n",
    "position = 0\n",
    "for driver, score in pred:\n",
    "    position += 1\n",
    "    print(f\"{driver} finishes in position:{position}      {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_posts = load_f1_df(2000)['text']\n",
    "display(validation_posts.head())\n",
    "validation_posts = validation_posts.apply(preprocessing.correct_spelling_in_text_spacy)\n",
    "validation_posts.to_csv(config.DATA_DIR /'file1.csv')\n",
    "# add new collumn with the label for each driver in the form: {'nyck de vries': {'positive': 0.0, 'neutral': 0.0, 'negative': 0.0, 'count': 0}, 'alexander albon': {'positive': 0.0, 'neutral': 0.0, 'negative': 0.0, 'count': 0}, 'yuki tsunoda': {'positive': 0.0057728992807760575, 'neutral': 0.9906835493288542, 'negative': 0.0035435524033872704, 'count': 57}, 'nicholas latifi': {'positive': 0.0, 'neutral': 0.0, 'negative': 0.0, 'count': 0}, 'lance stroll': {'positive': 0.0025615381891839206, 'neutral': 0.8542155921459198, 'negative': 0.1432228833436966, 'count': 2}, 'sergio perez': {'positive': 0.19108588388694617, 'neutral': 0.7905566372219701, 'negative': 0.018357482762380587, 'count': 19643}, 'liam lawson': {'positive': 0.0, 'neutral': 0.0, 'negative': 0.0, 'count': 0}, 'logan sargeant': {'positive': 0.0, 'neutral': 0.0, 'negative': 0.0, 'count': 0}, 'oscar piastri': {'positive': 0.0, 'neutral': 0.0, 'negative': 0.0, 'count': 0}, 'kevin magnussen': {'positive': 0.3014230728149414, 'neutral': 0.13589109480381012, 'negative': 0.5626858472824097, 'count': 119}, 'charles leclerc': {'positive': 0.3298702842287487, 'neutral': 0.6215965418101896, 'negative': 0.04853320448722903, 'count': 5804}, 'lewis hamilton': {'positive': 0.19249720936472856, 'neutral': 0.6415854867461116, 'negative': 0.1659173017253239, 'count': 602}, 'max verstappen': {'positive': 0.027957634713532145, 'neutral': 0.86048945951617, 'negative': 0.11155290395479907, 'count': 37483}, 'lando norris': {'positive': 0.10887042551651653, 'neutral': 0.8807938621942907, 'negative': 0.010335699243584025, 'count': 7665}, 'sebastian vettel': {'positive': 0.21298285713947993, 'neutral': 0.3937424868611979, 'negative': 0.39327468428606643, 'count': 9777}, 'mick schumacher': {'positive': 0.22496380869912483, 'neutral': 0.172746341228788, 'negative': 0.6022898651057537, 'count': 123}, 'fernando alonso': {'positive': 0.6737411749587325, 'neutral': 0.30806004730469894, 'negative': 0.018198754213155918, 'count': 8161}, 'zhou guanyu': {'positive': 0.003083703340962529, 'neutral': 0.9933833479881287, 'negative': 0.0035329661332070827, 'count': 5230}, 'nico hulkenberg': {'positive': 0.0, 'neutral': 0.0, 'negative': 0.0, 'count': 0}, 'carlos sainz': {'positive': 0.0, 'neutral': 0.0, 'negative': 0.0, 'count': 0}, 'esteban ocon': {'positive': 0.0, 'neutral': 0.0, 'negative': 0.0, 'count': 0}, 'daniel ricciardo': {'positive': 0.2623085604583092, 'neutral': 0.7047550600192255, 'negative': 0.03293641074422787, 'count': 5407}, 'valtteri bottas': {'positive': 0.0030204161646588666, 'neutral': 0.9952852886277141, 'negative': 0.0016942658217249356, 'count': 383}, 'george russell': {'positive': 0.019467041386267196, 'neutral': 0.24923343850033625, 'negative': 0.7312994546244864, 'count': 1848}, 'pierre gasly': {'positive': 0.15713243364682997, 'neutral': 0.7395840842833875, 'negative': 0.10328345954387029, 'count': 17877}}\n",
    "# load\n",
    "\n",
    "for i, gpt_df in enumerate(ChatGPT_posts):\n",
    "    comment = gpt_df[\"text\"].iloc[i]\n",
    "    score = gpt_df[\"score\"].iloc[i]\n",
    "\n",
    "    results = driver_sentiment(comment, F1_names, score)\n",
    "    # compare chatgpt column with results\n",
    "    # mae = abs()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
