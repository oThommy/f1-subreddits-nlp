{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from config.fastf1 import fastf1\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pandas.io.formats.style import Styler\n",
    "from collections.abc import Generator, Callable\n",
    "import typing\n",
    "from typing import Any, TypeAlias\n",
    "import numpy as np\n",
    "from contextlib import contextmanager\n",
    "from functools import partial, reduce\n",
    "import re\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import (\n",
    "    display, # type: ignore[reportUnknownVariableType]\n",
    "    Markdown,\n",
    ")\n",
    "\n",
    "import random\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "import importlib\n",
    "\n",
    "from config import config\n",
    "importlib.reload(config);\n",
    "\n",
    "import src.utils\n",
    "importlib.reload(src.utils);\n",
    "from src.utils import (\n",
    "    temporary_pandas_options,\n",
    "    display_full_dataframe,\n",
    "    hide_index,\n",
    "    compose,\n",
    ")\n",
    "\n",
    "import logging\n",
    "logging.getLogger('fastf1').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "display(Markdown('### r/formula1 submissions:'), f1_submissions_df.head(n))\n",
    "display(Markdown('### r/formula1 comments:'), f1_comments_df.head(n))\n",
    "display(Markdown('### r/formula1point5 submissions:'), f15_submissions_df.head(n))\n",
    "display(Markdown('### r/formula1point5 comments:'), f15_comments_df.head(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_submissions_and_comments(submissions_df: pd.DataFrame, comments_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    _submissions_df = submissions_df.copy()\n",
    "\n",
    "    titles: pd.Series[str] = _submissions_df['title'].str.rstrip()\n",
    "    selftexts: pd.Series[str] = _submissions_df['selftext']\n",
    "    alphanumeric_pattern = re.compile(r'\\w')\n",
    "\n",
    "    # Concatenate submission's title and selftext into a single text for NLP analysis\n",
    "    # TODO: still a bit buggy: title='title', selftext='' -> text='title. ' with trailing space\n",
    "    _submissions_df['text'] = np.where(\n",
    "        titles.str[-1].map(lambda ch: alphanumeric_pattern.match(ch) is not None),\n",
    "        titles + '. ' + selftexts,\n",
    "        titles + ' ' + selftexts,\n",
    "    )\n",
    "    _submissions_df.drop(columns=['title', 'selftext'], inplace=True)\n",
    "\n",
    "    _comments_df = comments_df.copy()\n",
    "    _comments_df.rename(columns={'body': 'text'}, inplace=True)\n",
    "\n",
    "    df = pd.concat((_submissions_df, _comments_df), ignore_index=True)  \n",
    "    return df\n",
    "\n",
    "f1_df = concatenate_submissions_and_comments(f1_submissions_df, f1_comments_df)\n",
    "f15_df = concatenate_submissions_and_comments(f15_submissions_df, f15_comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(f1_df.head(), len(f1_df))\n",
    "display(f15_df.head(), len(f15_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Rule-Based Prediction Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "__load_ndjson = partial(_load_ndjson, sample_size=5000)\n",
    "\n",
    "df = concatenate_submissions_and_comments(\n",
    "    _load_submissions_df(RAW_FORMULA1_SUBMISSIONS_FILE, __load_ndjson),\n",
    "    _load_comments_df(RAW_FORMULA1_COMMENTS_FILE, __load_ndjson),\n",
    ")\n",
    "\n",
    "with open('f1_random_sample_texts.txt', 'w', encoding='utf-8') as file:\n",
    "    print(*map(json.dumps, df['text']), sep='\\n', file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained model: BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"f1_random_sample_texts.txt\", sep=\"\\n\", header=None, names=[\"text\"])\n",
    "data[\"label\"] = 0  # Assign labels (e.g., 0 = non-winner, 1 = winner)\n",
    "# Ensure labels align with discussions about drivers (manual or inferred labeling)\n",
    "\n",
    "# Preprocess\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "data[\"input_ids\"] = data[\"text\"].apply(\n",
    "    lambda x: tokenizer(x, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")[\"input_ids\"]\n",
    ")\n",
    "data[\"attention_mask\"] = data[\"text\"].apply(\n",
    "    lambda x: tokenizer(x, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")[\"attention_mask\"]\n",
    ")\n",
    "\n",
    "# Split data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    data[[\"input_ids\", \"attention_mask\"]].values,\n",
    "    data[\"label\"].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Prepare datasets\n",
    "class F1Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.texts[idx][0],\n",
    "            \"attention_mask\": self.texts[idx][1],\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "train_dataset = F1Dataset(train_texts, train_labels)\n",
    "val_dataset = F1Dataset(val_texts, val_labels)\n",
    "\n",
    "# Model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=20)  # 20 drivers\n",
    "\n",
    "# Training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = \"GroNLP/hateBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"path_to_your_fine_tuned_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Function to classify text\n",
    "def classify_text(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    # Perform inference\n",
    "    outputs = model(**inputs)\n",
    "    # Apply softmax to get probabilities\n",
    "    probs = F.softmax(outputs.logits, dim=-1)\n",
    "    # Get the predicted class\n",
    "    predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "    # Map the predicted class to label\n",
    "    label_map = {0: \"Not Prediction\", 1: \"Prediction\"}\n",
    "    return label_map[predicted_class], probs\n",
    "\n",
    "# Example usage\n",
    "text = \"I think the race will end with car number 7 in the lead.\"\n",
    "label, probabilities = classify_text(text)\n",
    "print(f\"Label: {label}, Probabilities: {probabilities}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive power evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# model_name = \"GroNLP/hateBERT\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "\n",
    "# # Load the fine-tuned model and tokenizer\n",
    "# model_name = \"path_to_your_fine_tuned_model\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Function to classify text\n",
    "def classify_text(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    # Perform inference\n",
    "    outputs = model(**inputs)\n",
    "    # Apply softmax to get probabilities\n",
    "    probs = F.softmax(outputs.logits, dim=-1)\n",
    "    # Get the predicted class\n",
    "    predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "    # Map the predicted class to label\n",
    "    label_map = {0: \"Not Prediction\", 1: \"Prediction\"}\n",
    "    return label_map[predicted_class], probs\n",
    "\n",
    "# Example usage\n",
    "text = \"I think the race will end with car number 7 in the lead.\"\n",
    "label, probabilities = classify_text(text)\n",
    "print(f\"Label: {label}, Probabilities: {probabilities}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive power evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_schedule = fastf1.get_event_schedule(config.Dataset.YEAR)\n",
    "with display_full_dataframe():\n",
    "    display(full_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = full_schedule[\n",
    "    (full_schedule['EventDate'] >= config.Dataset.START_DATE) &\n",
    "    (full_schedule['EventDate'] <= config.Dataset.END_DATE) &\n",
    "    (full_schedule['EventFormat'] == 'conventional') # TODO: Skip sprint weekends for now. Also include sprint weekends later\n",
    "]\n",
    "display(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Example\n",
    "posts_df = f1_df\n",
    "event = schedule.iloc[-1]\n",
    "first_post_at = typing.cast(dt.datetime, event['Session1DateUtc']) - dt.timedelta(days=1)\n",
    "last_post_at = typing.cast(dt.datetime, event['Session5DateUtc'])\n",
    "posts_df = posts_df[\n",
    "    (posts_df['created_utc'] >= first_post_at) &\n",
    "    (posts_df['created_utc'] <= last_post_at)\n",
    "]\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(posts_df.head(8), len(posts_df))\n",
    "\n",
    "race_session = event.get_session('Race')\n",
    "race_session.load(laps=False, telemetry=False, weather=False, messages=False)\n",
    "\n",
    "with display_full_dataframe():\n",
    "    top20 = pd.DataFrame(\n",
    "        {\n",
    "            'Pos': range(1, 21),\n",
    "            'DriverFullName': race_session.results['FullName'],\n",
    "        }\n",
    "    )\n",
    "    display(hide_index(top20))\n",
    "\n",
    "for index, event in enumerate(schedule):\n",
    "    last_five_events = schedule[index-5:index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for _, event in schedule.iterrows():\n",
    "    event_name = event['EventName']\n",
    "    event_date = event['EventDate']\n",
    "    print(f\"Event: {event_name} on {event_date}\")\n",
    "\n",
    "    # Retrieve session start times\n",
    "    for session_name in ['Practice 1', 'Practice 2', 'Practice 3', 'Qualifying', 'Race']:\n",
    "        session = event.get_session(session_name)\n",
    "        if session is not None:\n",
    "            print(f\"  {session_name} starts at {session.date}\")\n",
    "\n",
    "    # Load race session to get results\n",
    "    race = event.get_session('Race')\n",
    "    if race is not None:\n",
    "        race.load()\n",
    "        results = race.results\n",
    "        # Display top 20 results\n",
    "        top_20 = results.head(20)\n",
    "        print(\"  Top 20 Race Results:\")\n",
    "        for position, driver in top_20.iterrows():\n",
    "            print(f\"    Position {position + 1}: {driver['FullName']} ({driver['TeamName']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "text = \"Max verstappen is going to win\"\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=MODEL, tokenizer=MODEL)\n",
    "sentiment_task(text)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model_config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(config.MODELS_DIR / 'sentiment_model.pt')\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "# # TF\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "# text = \"Covid cases are increasing fast!\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)\n",
    "# scores = output[0][0].numpy()\n",
    "# scores = softmax(scores)\n",
    "# Print labels and scores\n",
    "print(scores)\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "print(ranking)\n",
    "for i in range(scores.shape[0]):\n",
    "    l = model_config.id2label[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import urllib.request\n",
    "import spacy\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "# Initialize spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize SymSpell\n",
    "MAX_DICTIONARY_EDIT_DISTANCE = 4\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=MAX_DICTIONARY_EDIT_DISTANCE, prefix_length=7)\n",
    "\n",
    "\n",
    "\n",
    "def download_file(path, url):\n",
    "    if not path.exists():\n",
    "        try:\n",
    "            print('INFO: downloading english word dictionary...')\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "            print('downloading complete!!! :)')\n",
    "        except Exception as error:\n",
    "            raise Exception('Download failed: {error}')\n",
    "\n",
    "\n",
    "# english_words_dictionary_file = config.DATA_DIR / 'english_words_dictionary.txt'\n",
    "# download_file(english_words_dictionary_file, 'https://raw.githubusercontent.com/wolfgarbe/SymSpell/refs/heads/master/SymSpell/frequency_bigramdictionary_en_243_342.txt')\n",
    "\n",
    "english_words_dictionary_file = config.DATA_DIR / 'english_words_dictionary.txt'\n",
    "download_file(english_words_dictionary_file, 'https://raw.githubusercontent.com/wolfgarbe/SymSpell/refs/heads/master/SymSpell/frequency_dictionary_en_82_765.txt')\n",
    "\n",
    "with open(english_words_dictionary_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        word, frequency = line.strip().split()\n",
    "        frequency = int(frequency)\n",
    "        sym_spell.create_dictionary_entry(word, frequency)\n",
    "\n",
    "\n",
    "    # *phrase, frequency = line.rsplit(\" \", 1)\n",
    "    # phrase = \" \".join(phrase)  # Reconstruct the phrase\n",
    "    # frequency = int(frequency)  # Convert frequency to an integer\n",
    "\n",
    "for word in F1_VOCABULARY:\n",
    "    sym_spell.create_dictionary_entry(word, sys.maxsize)\n",
    "\n",
    "# Function to correct spelling\n",
    "def correct_spelling_symspell(word):\n",
    "    suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=MAX_DICTIONARY_EDIT_DISTANCE)\n",
    "    return suggestions[0].term if suggestions else word\n",
    "\n",
    "# Function to correct spelling in a sentence using spaCy\n",
    "def correct_spelling_in_text_spacy(text):\n",
    "    \"\"\"\n",
    "    Correct spelling of words in a text while preserving the structure.\n",
    "    Non-alphabetic tokens like punctuation remain unchanged.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    corrected_tokens = [\n",
    "        # correct_spelling_symspell(token.text) if token.ent_type_ == 'PERSON' else token.text\n",
    "        correct_spelling_symspell(token.text) if token.is_alpha else token.text\n",
    "        for token in doc\n",
    "    ]\n",
    "    combined_names = [\n",
    "        combine_names(corrected_tokens)\n",
    "    ]\n",
    "\n",
    "    return ''.join([\n",
    "        corrected_tokens[i] + (token.whitespace_ if token.whitespace_ else '')\n",
    "        for i, token in enumerate(doc)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_spelling_in_text_spacy('Mx Verstappening and Charls Lecerc are, just like this, a very good (/bad) example of drivers... and, voilà!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_pattern = re.compile(r'Verstappen|Hamilton|Leclerc')\n",
    "\n",
    "filtered_f1_df = f1_df[f1_df[\"text\"].str.contains(driver_pattern)]\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(filtered_f1_df.head(3))\n",
    "    \n",
    "filtered_f1_df['text'] = filtered_f1_df['text'].apply(correct_spelling_in_text_spacy)\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(filtered_f1_df.head(3))\n",
    "    print(len(filtered_f1_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_scores = {\"Verstappen\": [], \"Hamilton\": [], \"Leclerc\": []}\n",
    "\n",
    "tokens = text.split()\n",
    "corrected_tokens = [correct_spelling_symspell(word) for word in tokens]\n",
    "preprocessed_text = \" \".join(combined_tokens)\n",
    "\n",
    "for text in tqdm(filtered_f1_df[\"text\"]):\n",
    "    # print(len(text))\n",
    "    if len(text) > 1493:\n",
    "        continue\n",
    "    sentiment_result = sentiment_task(text)\n",
    "    # print(sentiment_result)\n",
    "    for driver in driver_scores.keys():\n",
    "        if driver in text:\n",
    "            driver_scores[driver].append(sentiment_result[0]['score'])\n",
    "\n",
    "# Aggregate scores\n",
    "final_scores = {driver: np.mean(scores) for driver, scores in driver_scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(driver_scores)\n",
    "print(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Load the ABSA model and tokenizer\n",
    "model_name = \"yangheng/deberta-v3-base-absa-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# comments = ['Sainz is loving this upgraded car, good top 3 for the race tomorrow! I disagree with you, verstappening will definitely finish first. I think BOT will finish behind NOR, who will probably finish 7th. That\\'s my opinion at least... I predict that the RedBulls with finish 1-2. Nah, the Danish driver from Haas will almost certainly finish in points! Stroll on the podium and Vettel in points. I like cookies!']\n",
    "# driver_list = ['Sainz', 'verstappening', 'BOT', 'NOR', 'Bert', 'Ernie']\n",
    "comments = ['Carlos Sainz is loving this upgraded car, good top 3 for the race tomorrow! I disagree with you, Max Verstappen will definitely finish first. I think BOT will finish behind NOR, who will probably finish 7th. That\\'s my opinion at least... I predict that the RedBulls with finish 1-2. Nah, the Danish driver from Haas will almost certainly finish in points! Stroll on the podium and Vettel in points. I like cookies!']\n",
    "driver_list = ['Carlos Sainz', 'Max Verstappen']\n",
    "\n",
    "results = {driver: {\"positive\": 0.0, \"neutral\": 0.0, \"negative\": 0.0, \"count\": 0} for driver in driver_list}\n",
    "\n",
    "for comment in comments:\n",
    "    found_drivers = [driver for driver in driver_list if driver in comment]\n",
    "    \n",
    "    for aspect in found_drivers:\n",
    "        inputs = tokenizer(comment, aspect, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        scores = outputs.logits[0].numpy()\n",
    "        probabilities = softmax(scores)\n",
    "\n",
    "        results[aspect][\"positive\"] += probabilities[2]\n",
    "        results[aspect][\"neutral\"] += probabilities[1]\n",
    "        results[aspect][\"negative\"] += probabilities[0]\n",
    "        results[aspect][\"count\"] += 1\n",
    "\n",
    "for driver, sentiment in results.items():\n",
    "    if sentiment[\"count\"] > 0:\n",
    "        sentiment[\"positive\"] /= sentiment[\"count\"]\n",
    "        sentiment[\"neutral\"] /= sentiment[\"count\"]\n",
    "        sentiment[\"negative\"] /= sentiment[\"count\"]\n",
    "\n",
    "for driver, sentiment in results.items():\n",
    "    if sentiment[\"count\"] > 0:\n",
    "        print(f\"{driver}: [Positive: {sentiment['positive']:.4f}, Neutral: {sentiment['neutral']:.4f}, Negative: {sentiment['negative']:.4f}]\")\n",
    "    else:\n",
    "        print(f\"{driver}: No mentions found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores = []\n",
    "for driver, sentiment in results.items():\n",
    "    if sentiment[\"count\"] > 0:\n",
    "        sentiment_score = (sentiment[\"positive\"] - sentiment[\"negative\"])\n",
    "        final_scores.append((driver.title(), sentiment_score))\n",
    "\n",
    "# Sort drivers by positive - negative score (descending order)\n",
    "final_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print sorted results\n",
    "print(\"Drivers ranked by (positive - negative score):\")\n",
    "for driver, score in final_scores:\n",
    "    print(f\"{driver}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load last 5 races\n",
    "\n",
    "historical_data = top20\n",
    "historical_scores = {row[\"DriverFullName\"]: 0 for _, row in historical_data.iterrows()}\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    historical_data = top20\n",
    "\n",
    "    for _, row in historical_data.iterrows():\n",
    "        historical_scores[row[\"DriverFullName\"]] += 1 - ((row[\"Pos\"] - 1) / 19) * 2\n",
    "\n",
    "for driver, score in historical_scores.items():\n",
    "    historical_scores[driver] = score / 5\n",
    "\n",
    "final_scores_dict = dict(final_scores)\n",
    "\n",
    "print(final_scores_dict)\n",
    "print(historical_scores)\n",
    "\n",
    "final_prediction = []\n",
    "for driver, historical_score in historical_scores.items():\n",
    "    if driver in final_scores_dict:\n",
    "        score = final_scores_dict[driver]\n",
    "        \n",
    "        combined_score = 0.6 * score + 0.4 * historical_score\n",
    "        final_prediction.append((driver, combined_score))\n",
    "    else:\n",
    "        final_prediction.append((driver, historical_score))\n",
    "\n",
    "final_prediction.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "pos = 0\n",
    "for driver, score in final_prediction:\n",
    "    pos += 1\n",
    "    print(f\"{driver} finishes in position:{pos}      {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLiNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gliner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"C:\\\\cache\"\n",
    "from gliner import GLiNER\n",
    "model = GLiNER.from_pretrained('urchade/gliner_medium-v2.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Carlos Sainz is loving this upgraded car, good top 3 for the race tomorrow! I disagree with you, verstappening will definitely finish first. I think BOT will finish behind NOR, who will probably finish 7th. That\\'s my opinion at least... I predict that the RedBulls with finish 1-2. Nah, the Danish driver from Haas will almost certainly finish in points! Stroll on the podium and Vettel in points. I like cookies!'\n",
    "\n",
    "df = f1_df.iloc[:2]\n",
    "\n",
    "\n",
    "def has_prediction(post: pd.Series, threshold: float = 0.01) -> bool:\n",
    "    position_entities = model.predict_entities(post['text'], ['position'], threshold=threshold) # TODO: very low threshold\n",
    "    \n",
    "    return len(position_entities) != 0\n",
    "\n",
    "df['text'] = df['text'].apply(has_prediction)\n",
    "display(df)\n",
    "\n",
    "labels = ['racer', 'position']\n",
    "\n",
    "entities = model.predict_entities(text, labels, threshold=0.01) # TODO: very low threshold\n",
    "\n",
    "for entity in entities:\n",
    "    print(entity['text'], '=>', entity['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
