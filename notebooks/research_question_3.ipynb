{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from collections.abc import Generator, Callable\n",
    "from pathlib import Path\n",
    "import typing\n",
    "from typing import Any, TypeAlias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import re\n",
    "from functools import partial, reduce\n",
    "from tqdm import tqdm\n",
    "from IPython.display import (\n",
    "    display, # type: ignore[reportUnknownVariableType]\n",
    "    Markdown,\n",
    ")\n",
    "\n",
    "import importlib\n",
    "\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize as tokenize_nltk\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from config.fastf1 import fastf1\n",
    "from config import config\n",
    "from src.data.loader import stream_ndjson, load_submissions_df, load_comments_df\n",
    "from src.data import preprocessing\n",
    "importlib.reload(preprocessing)\n",
    "import src.data.constants as dataset_constants\n",
    "\n",
    "from src.utils import (\n",
    "    temporary_pandas_options,\n",
    "    display_full_dataframe,\n",
    "    hide_index,\n",
    "    compose,\n",
    ")\n",
    "from src import utils\n",
    "utils.set_random_seeds()\n",
    "\n",
    "import logging\n",
    "logging.getLogger('fastf1').setLevel(logging.WARNING)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_ndjson_streamer = partial(stream_ndjson, limit=10000)\n",
    "# f1_ndjson_streamer = partial(stream_ndjson)\n",
    "\n",
    "f1_submissions_df = load_submissions_df(dataset_constants.RawFile.FORMULA1_SUBMISSIONS, f1_ndjson_streamer)\n",
    "f1_comments_df = load_comments_df(dataset_constants.RawFile.FORMULA1_COMMENTS, f1_ndjson_streamer)\n",
    "\n",
    "f15_submissions_df = load_submissions_df(dataset_constants.RawFile.FORMULA1POINT5_SUBMISSIONS)\n",
    "f15_comments_df = load_comments_df(dataset_constants.RawFile.FORMULA1POINT5_COMMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(Markdown('### r/formula1 submissions:'), f1_submissions_df.head(n))\n",
    "    display(Markdown('### r/formula1 comments:'), f1_comments_df.head(n))\n",
    "    display(Markdown('### r/formula1point5 submissions:'), f15_submissions_df.head(n))\n",
    "    display(Markdown('### r/formula1point5 comments:'), f15_comments_df.head(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df = preprocessing.concatenate_submissions_and_comments(f1_submissions_df, f1_comments_df)\n",
    "f15_df = preprocessing.concatenate_submissions_and_comments(f15_submissions_df, f15_comments_df)\n",
    "\n",
    "n = 3\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(Markdown('### r/formula1 posts:'), f1_df.head(n))\n",
    "    display(Markdown('### r/formula1point5 posts:'), f15_df.head(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict, Counter\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "\n",
    "processed_testdata = [\n",
    "    ['ricciardo', 'to', 'red', 'bull'],\n",
    "    ['hamilton', 'to', 'stay', 'mercedes'],\n",
    "    ['alonso', 'to', 'aston', 'martin'],\n",
    "    ['max', 'verstappen', 'to', 'ferrari'],\n",
    "    ['max', 'verstappen', 'stay', 'red', 'bull'],\n",
    "    ['max', 'verstappen', 'stay', 'by', 'red', 'bull']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.blank('en')\n",
    "# tokenize_spacy = nlp.tokenizer\n",
    "\n",
    "# f1_df['text'] = f1_df['text'].apply(partial(preprocessing.correct_spelling_in_text_spacy, activator=False))\n",
    "# f1_df['text'] = f1_df['text'].apply(preprocessing.normalize)\n",
    "# f1_df['text'] = f1_df['text'].apply(preprocessing.lemmatize)\n",
    "\n",
    "# tokenized_texts = [list(map(lambda token: token.text, tokenize_spacy(text))) for text in normalized_texts]\n",
    "\n",
    "\n",
    "f1_df['text'] = f1_df['text'].apply(preprocessing.normalize)\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "all_lemmatized_tokens = [\n",
    "    [token.lemma_.lower() for token in nlp(text)] \n",
    "    for text in f1_df['text']\n",
    "]\n",
    "\n",
    "print(all_lemmatized_tokens[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with display_full_dataframe():\n",
    "#    display(f1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lists of drivers and teams\n",
    "\n",
    "drivers = [\n",
    "    'max', 'verstappen',\n",
    "    'charles', 'leclerc',\n",
    "    'sergio', 'perez',\n",
    "    'george', 'russell',\n",
    "    'carlos', 'sainz',\n",
    "    'lewis', 'hamilton',\n",
    "    'lando', 'norris',\n",
    "    'esteban', 'ocon',\n",
    "    'fernando', 'alonso',\n",
    "    'valtteri', 'bottas',\n",
    "    'daniel', 'ricciardo',\n",
    "    'sebastian', 'vettel',\n",
    "    'kevin', 'magnussen',\n",
    "    'pierre', 'gasly',\n",
    "    'lance', 'stroll',\n",
    "    'mick', 'schumacher',\n",
    "    'yuki', 'tsunoda',\n",
    "    'zhou', 'guanyu',\n",
    "    'alexander', 'albon',\n",
    "    'nicholas', 'latifi',\n",
    "    'nyck', 'vries',\n",
    "    'nico', 'hulkenberg',\n",
    "    'oscar', 'piastri',\n",
    "    'liam', 'lawson',\n",
    "    'logan', 'sargeant'\n",
    "]\n",
    "\n",
    "teams = [\n",
    "    'mercedes',\n",
    "    'ferrari',\n",
    "    'red', 'bull',\n",
    "    'alpine', 'renault',\n",
    "    'mclaren',\n",
    "    'aston', 'martin',\n",
    "    'racing', 'point',\n",
    "    'alphatauri', 'alpha', 'tauri',\n",
    "    'haas',\n",
    "    'alfa', 'romeo',\n",
    "    'williams',\n",
    "    'kick', 'sauber'\n",
    "]\n",
    "\n",
    "action_words = [\n",
    "    'to',\n",
    "    'go',\n",
    "    'goes',\n",
    "    'leave',\n",
    "    'leaves',\n",
    "    'join',\n",
    "    'joins',\n",
    "    'sign',\n",
    "    'signs',\n",
    "    'extend',\n",
    "    'extends',\n",
    "    'move',\n",
    "    'moves',\n",
    "    'replace',\n",
    "    'replaces',\n",
    "    'return',\n",
    "    'returns',\n",
    "    'stay',\n",
    "    'stays'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Filter sentences containing both a driver, a team and a action word\n",
    "'''\n",
    "def filter_sentences_by_driver_and_team(tokenized_texts, drivers, teams):\n",
    "    filtered_sentences = []\n",
    "    for sentence in tokenized_texts:\n",
    "        contains_driver = any(driver in sentence for driver in drivers)\n",
    "        contains_team = any(team in sentence for team in teams)\n",
    "        contains_action_word = any(action in sentence for action in action_words)\n",
    "        if contains_driver and contains_team and contains_action_word:\n",
    "            filtered_sentences.append(sentence)\n",
    "    return filtered_sentences\n",
    "'''\n",
    "\n",
    "def filter_sentences_by_driver_and_team(tokenized_texts, drivers, teams, action_words):\n",
    "    filtered_sentences = []\n",
    "\n",
    "    for sentence in tokenized_texts:\n",
    "        contains_team = any(team in sentence for team in teams)\n",
    "        \n",
    "        # Check if any driver is followed by an action word\n",
    "        for i, word in enumerate(sentence):\n",
    "            if word in drivers and i + 1 < len(sentence) and sentence[i + 1] in action_words:\n",
    "                if contains_team:  # Ensure a team is mentioned anywhere\n",
    "                    filtered_sentences.append(sentence)\n",
    "                    break  # Move to the next sentence after finding a match\n",
    "\n",
    "    return filtered_sentences\n",
    "\n",
    "# Apply the filter\n",
    "filtered_sentences = filter_sentences_by_driver_and_team(all_lemmatized_tokens, drivers, teams, action_words)\n",
    "\n",
    "print(all_lemmatized_tokens[:5])  # Check the first 5 tokenized sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create N-gram model\n",
    "\n",
    "def train_ngram_model(data, n=2):\n",
    "    ngram_counts = defaultdict(Counter)\n",
    "    total_counts = Counter()\n",
    "\n",
    "    for sentence in data:\n",
    "        sentence = ['<s>'] + sentence + ['</s>']  # Add start and end tokens\n",
    "        n_grams = list(ngrams(sentence, n))\n",
    "        for gram in n_grams:\n",
    "            prefix, next_word = tuple(gram[:-1]), gram[-1]\n",
    "            ngram_counts[prefix][next_word] += 1\n",
    "            total_counts[prefix] += 1\n",
    "\n",
    "    # Convert counts to probabilities\n",
    "    ngram_probs = {\n",
    "        prefix: {word: count / total_counts[prefix] for word, count in words.items()}\n",
    "        for prefix, words in ngram_counts.items()\n",
    "    }\n",
    "\n",
    "    return ngram_probs\n",
    "\n",
    "\n",
    "\n",
    "# Train a bigram model\n",
    "bigram_model = train_ngram_model(filtered_sentences, n=2)\n",
    "\n",
    "# Train a trigram model\n",
    "trigram_model = train_ngram_model(filtered_sentences, n=3)\n",
    "\n",
    "# Train a quadgram model\n",
    "quadgram_model = train_ngram_model(filtered_sentences, n=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trigram model with Laplace Smoothing\n",
    "\n",
    "def train_trigram_model_with_smoothing(data, n=3):\n",
    "    ngram_counts = defaultdict(Counter)\n",
    "    total_counts = Counter()\n",
    "    vocabulary = set()\n",
    "\n",
    "    for sentence in data:\n",
    "        sentence = ['<s>'] * (n - 1) + sentence + ['</s>']  # Add padding\n",
    "        n_grams = list(ngrams(sentence, n))\n",
    "        vocabulary.update(sentence)  # Add tokens to vocabulary\n",
    "        for gram in n_grams:\n",
    "            prefix, next_word = tuple(gram[:-1]), gram[-1]\n",
    "            ngram_counts[prefix][next_word] += 1\n",
    "            total_counts[prefix] += 1\n",
    "\n",
    "    # Laplace Smoothing\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    trigram_probs = {\n",
    "        prefix: {word: (count + 1) / (total_counts[prefix] + vocabulary_size)\n",
    "                 for word, count in words.items()}\n",
    "        for prefix, words in ngram_counts.items()\n",
    "    }\n",
    "\n",
    "    # Ensure all words in the vocabulary have a non-zero probability\n",
    "    for prefix in ngram_counts.keys():\n",
    "        for word in vocabulary:\n",
    "            if word not in trigram_probs[prefix]:\n",
    "                trigram_probs[prefix][word] = 1 / (total_counts[prefix] + vocabulary_size)\n",
    "\n",
    "    return trigram_probs, vocabulary\n",
    "\n",
    "\n",
    "\n",
    "# Train a trigram model with Laplace Smoothing\n",
    "trigram_model_s, vocabulary = train_trigram_model_with_smoothing(filtered_sentences, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Next Word\n",
    "\n",
    "def predict_next_word(model, input_text, n=2):\n",
    "    tokens = input_text.lower().split()\n",
    "    prefix = tuple(tokens[-(n-1):])  # Use last (n-1) words as prefix\n",
    "    if prefix in model:\n",
    "        return max(model[prefix], key=model[prefix].get)  # Return word with highest probability\n",
    "    else:\n",
    "        return \"<unk>\"  # Return unknown token if prefix not found\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "input_text = \"daniel ricciardo\"\n",
    "next_word = predict_next_word(bigram_model, input_text, n=2)\n",
    "print(f\"Next word: {next_word}\")\n",
    "\n",
    "input_text = \"lewis hamilton\"\n",
    "next_word = predict_next_word(bigram_model, input_text, n=2)\n",
    "print(f\"Next word: {next_word}\")\n",
    "\n",
    "input_text = \"daniel ricciardo\"\n",
    "next_word = predict_next_word(trigram_model, input_text, n=3)\n",
    "print(f\"Next word: {next_word}\")\n",
    "\n",
    "input_text = \"lewis hamilton\"\n",
    "next_word = predict_next_word(trigram_model, input_text, n=3)\n",
    "print(f\"Next word: {next_word}\")\n",
    "\n",
    "input_text = \"max verstappen\"\n",
    "next_word = predict_next_word(trigram_model, input_text, n=3)\n",
    "print(f\"Next word: {next_word}\")\n",
    "\n",
    "input_text = \"max verstappen to\"\n",
    "next_word = predict_next_word(quadgram_model, input_text, n=4)\n",
    "print(f\"Next word: {next_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate full predictions\n",
    "\n",
    "def generate_predictions(model, seed_text, n=2, max_length=10):\n",
    "    tokens = seed_text.lower().split()\n",
    "    for _ in range(max_length):\n",
    "        next_word = predict_next_word(model, \" \".join(tokens), n=n)\n",
    "        if next_word == \"</s>\":\n",
    "            break\n",
    "        tokens.append(next_word)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "\n",
    "# Generate a prediction\n",
    "\n",
    "seed_text = \"daniel ricciardo to\"\n",
    "prediction = generate_predictions(bigram_model, seed_text, n=2)\n",
    "print(f\"Generated prediction: {prediction}\")\n",
    "\n",
    "seed_text = \"lewis hamilton to\"\n",
    "prediction = generate_predictions(bigram_model, seed_text, n=2)\n",
    "print(f\"Generated prediction: {prediction}\")\n",
    "\n",
    "seed_text = \"daniel ricciardo to\"\n",
    "prediction = generate_predictions(trigram_model, seed_text, n=3)\n",
    "print(f\"Generated prediction: {prediction}\")\n",
    "\n",
    "seed_text = \"lewis hamilton to\"\n",
    "prediction = generate_predictions(trigram_model, seed_text, n=3)\n",
    "print(f\"Generated prediction: {prediction}\")\n",
    "\n",
    "seed_text = \"max verstappen to\"\n",
    "prediction = generate_predictions(trigram_model, seed_text, n=3)\n",
    "print(f\"Generated prediction: {prediction}\")\n",
    "\n",
    "seed_text = \"sergio perez to\"\n",
    "prediction = generate_predictions(trigram_model, seed_text, n=3)\n",
    "print(f\"Generated prediction: {prediction}\")\n",
    "\n",
    "seed_text = \"perez to\"\n",
    "prediction = generate_predictions(trigram_model_s, seed_text, n=3)\n",
    "print(f\"Generated prediction: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_team(model, teams, input_text, n=3):\n",
    "    \"\"\"\n",
    "    Predict the next word from the model, forcing it to be a team name.\n",
    "    \n",
    "    Args:\n",
    "    - model: Trained N-gram model with probabilities.\n",
    "    - teams: Set of valid team names.\n",
    "    - input_text: Input text string (e.g., \"ricciardo joins\").\n",
    "    - n: N-gram size (default: 3).\n",
    "    \n",
    "    Returns:\n",
    "    - Predicted team name or \"<unk>\" if no team matches.\n",
    "    \"\"\"\n",
    "    tokens = input_text.lower().split()\n",
    "    prefix = tuple(tokens[-(n - 1):])  # Last (n-1) words as the prefix\n",
    "    \n",
    "    if prefix in model:\n",
    "        # Filter predictions to include only team names\n",
    "        team_predictions = {word: prob for word, prob in model[prefix].items() if word in teams}\n",
    "        if team_predictions:\n",
    "            return max(team_predictions, key=team_predictions.get)  # Team with highest probability\n",
    "    return \"<unk>\"  # Return \"<unk>\" if no team matches\n",
    "\n",
    "\n",
    "\n",
    "input_text = \"ricciardo join\"\n",
    "next_word = predict_next_team(trigram_model, teams, input_text, n=3)\n",
    "print(f\"Next predicted team: {next_word}\")\n",
    "\n",
    "input_text = \"verstappen to\"\n",
    "next_word = predict_next_team(trigram_model, teams, input_text, n=3)\n",
    "print(f\"Next predicted team: {next_word}\")\n",
    "\n",
    "input_text = \"hamilton to\"\n",
    "next_word = predict_next_team(trigram_model, teams, input_text, n=3)\n",
    "print(f\"Next predicted team: {next_word}\")\n",
    "\n",
    "input_text = \"sergio perez to\"\n",
    "next_word = predict_next_team(trigram_model, teams, input_text, n=3)\n",
    "print(f\"Next predicted team: {next_word}\")\n",
    "\n",
    "input_text = \"leclerc join\"\n",
    "next_word = predict_next_team(trigram_model, teams, input_text, n=3)\n",
    "print(f\"Next predicted team: {next_word}\")\n",
    "\n",
    "input_text = \"alonso join\"\n",
    "next_word = predict_next_team(trigram_model, teams, input_text, n=3)\n",
    "print(f\"Next predicted team: {next_word}\")\n",
    "\n",
    "input_text = \"vettel to\"\n",
    "next_word = predict_next_team(trigram_model, teams, input_text, n=3)\n",
    "print(f\"Next predicted team: {next_word}\")\n",
    "\n",
    "input_text = \"max verstappen will stay at red\"\n",
    "next_word = predict_next_team(quadgram_model, teams, input_text, n=4)\n",
    "print(f\"Next predicted team: {next_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_top_n_for_input(model, input_text, n=5, ngram_size=3):\n",
    "\n",
    "    tokens = tokens = input_text.split()\n",
    "    prefix = tuple(tokens[-(ngram_size - 1):])  # Get the last (n-1) tokens\n",
    "\n",
    "    if prefix in model:\n",
    "        # Sort predictions by probability and extract the top N\n",
    "        top_predictions = sorted(model[prefix].items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "        # Create DataFrame for easy display\n",
    "        df = pd.DataFrame(top_predictions, columns=[\"Word\", \"Probability\"])\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Prefix not found in the model. No predictions available.\")\n",
    "        return pd.DataFrame(columns=[\"Word\", \"Probability\"])\n",
    "\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "input_text = \"max to\"\n",
    "top_predictions = predict_top_n_for_input(trigram_model, input_text, n=5)\n",
    "print(top_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_summary = dict()\n",
    "\n",
    "def sort_to_team(team, driver):\n",
    "\n",
    "    if team not in team_summary:\n",
    "        team_summary[team] = {driver}\n",
    "    else:\n",
    "        team_summary[team].add(driver)\n",
    "\n",
    "\n",
    "def print_team_summary(summary):\n",
    "   \n",
    "    print(\"Team Summary:\")\n",
    "    for team, drivers in summary.items():\n",
    "        print(f\"  {team}: {', '.join(drivers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"max verstappen to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"charles leclerc to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"sergio perez to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"george russell to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"carlos sainz to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"lewis hamilton to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"lando norris to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"esteban ocon to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"fernando alonso to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"valtteri bottas to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"daniel ricciardo to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"sebastian vettel to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"kevin magnussen to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"pierre gasly to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"lance stroll to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"mick schumacher to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"yuki tsunoda to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"zhou guanyu to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"alexander albon to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"nicholas latifi to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"nyck de vries to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"nico hulkenberg to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"oscar piastri to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"liam lawson to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n",
    "\n",
    "input_text = \"logan sargeant to\"\n",
    "next_word = predict_next_team(trigram_model_s, teams, input_text, n=3)\n",
    "print(f\"Next predicted team for {input_text[:-3]}: {next_word}\")\n",
    "sort_to_team(next_word, input_text[:-3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_team_summary(team_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
