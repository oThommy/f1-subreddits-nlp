{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from collections.abc import Generator, Callable\n",
    "from pathlib import Path\n",
    "import typing\n",
    "from typing import Any, TypeAlias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import re\n",
    "from functools import partial, reduce\n",
    "from tqdm import tqdm\n",
    "from IPython.display import (\n",
    "    display, # type: ignore[reportUnknownVariableType]\n",
    "    Markdown,\n",
    ")\n",
    "\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize as tokenize_nltk\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from config.fastf1 import fastf1\n",
    "from config import config\n",
    "from src.data.loader import stream_ndjson, load_submissions_df, load_comments_df\n",
    "from src.data import preprocessing\n",
    "import src.data.constants as dataset_constants\n",
    "\n",
    "from src.utils import (\n",
    "    temporary_pandas_options,\n",
    "    display_full_dataframe,\n",
    "    hide_index,\n",
    "    compose,\n",
    ")\n",
    "from src import utils\n",
    "utils.set_random_seeds()\n",
    "\n",
    "import logging\n",
    "logging.getLogger('fastf1').setLevel(logging.WARNING)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_ndjson_streamer = partial(stream_ndjson, limit=5000)\n",
    "\n",
    "f1_submissions_df = load_submissions_df(dataset_constants.RawFile.FORMULA1_SUBMISSIONS, f1_ndjson_streamer)\n",
    "f1_comments_df = load_comments_df(dataset_constants.RawFile.FORMULA1_COMMENTS, f1_ndjson_streamer)\n",
    "\n",
    "f15_submissions_df = load_submissions_df(dataset_constants.RawFile.FORMULA1POINT5_SUBMISSIONS)\n",
    "f15_comments_df = load_comments_df(dataset_constants.RawFile.FORMULA1POINT5_COMMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(Markdown('### r/formula1 submissions:'), f1_submissions_df.head(n))\n",
    "    display(Markdown('### r/formula1 comments:'), f1_comments_df.head(n))\n",
    "    display(Markdown('### r/formula1point5 submissions:'), f15_submissions_df.head(n))\n",
    "    display(Markdown('### r/formula1point5 comments:'), f15_comments_df.head(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df = preprocessing.concatenate_submissions_and_comments(f1_submissions_df, f1_comments_df)\n",
    "f15_df = preprocessing.concatenate_submissions_and_comments(f15_submissions_df, f15_comments_df)\n",
    "\n",
    "n = 3\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(Markdown('### r/formula1 posts:'), f1_df.head(n))\n",
    "    display(Markdown('### r/formula1point5 posts:'), f15_df.head(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict, Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "\n",
    "processed_testdata = [\n",
    "    ['ricciardo', 'to', 'red', 'bull'],\n",
    "    ['hamilton', 'to', 'stay', 'mercedes'],\n",
    "    ['alonso', 'to', 'aston', 'martin'],\n",
    "    ['max', 'verstappen', 'to', 'ferrari'],\n",
    "    ['max', 'verstappen', 'stay', 'red', 'bull'],\n",
    "    ['max', 'verstappen', 'stay', 'by', 'red', 'bull']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('en')\n",
    "tokenize_spacy = nlp.tokenizer\n",
    "normalized_texts = list(f1_df['text'].apply(preprocessing.normalize))\n",
    "\n",
    "\n",
    "# tokenized_texts = [list(map(lambda token: token.text, tokenize_spacy(text))) for text in normalized_texts]\n",
    "tokenized_texts = [tokenize_nltk(text) for text in normalized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lists of drivers and teams\n",
    "\n",
    "drivers = [\n",
    "    'max', 'verstappen',\n",
    "    'charles', 'leclerc',\n",
    "    'sergio', 'perez',\n",
    "    'george', 'russell',\n",
    "    'carlos', 'sainz',\n",
    "    'lewis', 'hamilton',\n",
    "    'lando', 'norris',\n",
    "    'esteban', 'ocon',\n",
    "    'fernando', 'alonso',\n",
    "    'valtteri', 'bottas',\n",
    "    'daniel', 'ricciardo',\n",
    "    'sebastian', 'vettel',\n",
    "    'kevin', 'magnussen',\n",
    "    'pierre', 'gasly',\n",
    "    'lance', 'stroll',\n",
    "    'mick', 'schumacher',\n",
    "    'yuki', 'tsunoda',\n",
    "    'zhou', 'guanyu',\n",
    "    'alexander', 'albon',\n",
    "    'nicholas', 'latifi',\n",
    "    'nyck', 'vries',\n",
    "    'nico', 'hulkenberg',\n",
    "    'oscar', 'piastri',\n",
    "    'liam', 'lawson',\n",
    "    'logan', 'sargeant'\n",
    "]\n",
    "\n",
    "teams = [\n",
    "    'mercedes',\n",
    "    'ferrari',\n",
    "    'red', 'bull',\n",
    "    'alpine', 'renault',\n",
    "    'mclaren',\n",
    "    'aston', 'martin',\n",
    "    'racing', 'point',\n",
    "    'alphatauri', 'alpha', 'tauri',\n",
    "    'haas',\n",
    "    'alfa', 'romeo',\n",
    "    'williams',\n",
    "    'kick', 'sauber'\n",
    "]\n",
    "\n",
    "action_words = [\n",
    "    'go',\n",
    "    'goes',\n",
    "    'leave',\n",
    "    'leaves',\n",
    "    'join',\n",
    "    'joins',\n",
    "    'sign',\n",
    "    'signs',\n",
    "    'extend',\n",
    "    'extends',\n",
    "    'move',\n",
    "    'moves',\n",
    "    'replace',\n",
    "    'replaces',\n",
    "    'return',\n",
    "    'returns',\n",
    "    'stay',\n",
    "    'stays'\n",
    "]\n",
    "\n",
    "# Filter sentences containing both a driver and a team\n",
    "def filter_sentences_by_driver_and_team(tokenized_texts, drivers, teams):\n",
    "    filtered_sentences = []\n",
    "    for sentence in tokenized_texts:\n",
    "        contains_driver = any(driver in sentence for driver in drivers)\n",
    "        contains_team = any(team in sentence for team in teams)\n",
    "        contains_action_word = any(action in sentence for action in action_words)\n",
    "        if contains_driver and contains_team and contains_action_word:\n",
    "            filtered_sentences.append(sentence)\n",
    "    return filtered_sentences\n",
    "\n",
    "\n",
    "# Apply the filter\n",
    "filtered_sentences = filter_sentences_by_driver_and_team(tokenized_texts, drivers, teams)\n",
    "\n",
    "print(filtered_sentences[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create N-gram model\n",
    "\n",
    "def train_ngram_model(data, n=2):\n",
    "    ngram_counts = defaultdict(Counter)\n",
    "    total_counts = Counter()\n",
    "\n",
    "    for sentence in data:\n",
    "        sentence = ['<s>'] + sentence + ['</s>']  # Add start and end tokens\n",
    "        n_grams = list(ngrams(sentence, n))\n",
    "        for gram in n_grams:\n",
    "            prefix, next_word = tuple(gram[:-1]), gram[-1]\n",
    "            ngram_counts[prefix][next_word] += 1\n",
    "            total_counts[prefix] += 1\n",
    "\n",
    "    # Convert counts to probabilities\n",
    "    ngram_probs = {\n",
    "        prefix: {word: count / total_counts[prefix] for word, count in words.items()}\n",
    "        for prefix, words in ngram_counts.items()\n",
    "    }\n",
    "\n",
    "    return ngram_probs\n",
    "\n",
    "\n",
    "\n",
    "# Train a bigram model\n",
    "bigram_model = train_ngram_model(filtered_sentences, n=2)\n",
    "\n",
    "# Train a trigram model\n",
    "trigram_model = train_ngram_model(filtered_sentences, n=3)\n",
    "\n",
    "# Train a quadgram model\n",
    "quadgram_model = train_ngram_model(filtered_sentences, n=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trigram model with Laplace Smoothing\n",
    "\n",
    "def train_trigram_model_with_smoothing(data, n=3):\n",
    "    ngram_counts = defaultdict(Counter)\n",
    "    total_counts = Counter()\n",
    "    vocabulary = set()\n",
    "\n",
    "    for sentence in data:\n",
    "        sentence = ['<s>'] * (n - 1) + sentence + ['</s>']  # Add padding\n",
    "        n_grams = list(ngrams(sentence, n))\n",
    "        vocabulary.update(sentence)  # Add tokens to vocabulary\n",
    "        for gram in n_grams:\n",
    "            prefix, next_word = tuple(gram[:-1]), gram[-1]\n",
    "            ngram_counts[prefix][next_word] += 1\n",
    "            total_counts[prefix] += 1\n",
    "\n",
    "    # Laplace Smoothing\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    trigram_probs = {\n",
    "        prefix: {word: (count + 1) / (total_counts[prefix] + vocabulary_size)\n",
    "                 for word, count in words.items()}\n",
    "        for prefix, words in ngram_counts.items()\n",
    "    }\n",
    "\n",
    "    # Ensure all words in the vocabulary have a non-zero probability\n",
    "    for prefix in ngram_counts.keys():\n",
    "        for word in vocabulary:\n",
    "            if word not in trigram_probs[prefix]:\n",
    "                trigram_probs[prefix][word] = 1 / (total_counts[prefix] + vocabulary_size)\n",
    "\n",
    "    return trigram_probs, vocabulary\n",
    "\n",
    "\n",
    "\n",
    "# Train a trigram model with Laplace Smoothing\n",
    "trigram_model_s = train_trigram_model_with_smoothing(filtered_sentences, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Next Word\n",
    "\n",
    "def predict_next_word(model, input_text, n=2):\n",
    "    tokens = input_text.lower().split()\n",
    "    prefix = tuple(tokens[-(n-1):])  # Use last (n-1) words as prefix\n",
    "    if prefix in model:\n",
    "        return max(model[prefix], key=model[prefix].get)  # Return word with highest probability\n",
    "    else:\n",
    "        return \"<unk>\"  # Return unknown token if prefix not found\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "input_text = \"daniel ricciardo\"\n",
    "next_word = predict_next_word(bigram_model, input_text, n=2)\n",
    "print(f\"Next word: {next_word}\")\n",
    "\n",
    "input_text = \"lewis hamilton\"\n",
    "next_word = predict_next_word(bigram_model, input_text, n=2)\n",
    "print(f\"Next word: {next_word}\")\n",
    "\n",
    "input_text = \"daniel ricciardo\"\n",
    "next_word = predict_next_word(trigram_model, input_text, n=3)\n",
    "print(f\"Next word: {next_word}\")\n",
    "\n",
    "input_text = \"lewis hamilton\"\n",
    "next_word = predict_next_word(trigram_model, input_text, n=3)\n",
    "print(f\"Next word: {next_word}\")\n",
    "\n",
    "input_text = \"max verstappen\"\n",
    "next_word = predict_next_word(trigram_model, input_text, n=3)\n",
    "print(f\"Next word: {next_word}\")\n",
    "\n",
    "input_text = \"max verstappen stays\"\n",
    "next_word = predict_next_word(quadgram_model, input_text, n=4)\n",
    "print(f\"Next word: {next_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate full predictions\n",
    "\n",
    "def generate_predictions(model, seed_text, n=2, max_length=10):\n",
    "    tokens = seed_text.lower().split()\n",
    "    for _ in range(max_length):\n",
    "        next_word = predict_next_word(model, \" \".join(tokens), n=n)\n",
    "        if next_word == \"</s>\":\n",
    "            break\n",
    "        tokens.append(next_word)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "\n",
    "# Generate a prediction\n",
    "\n",
    "seed_text = \"daniel ricciardo\"\n",
    "prediction = generate_predictions(bigram_model, seed_text, n=2)\n",
    "print(f\"Generated prediction: {prediction}\")\n",
    "\n",
    "seed_text = \"lewis hamilton\"\n",
    "prediction = generate_predictions(bigram_model, seed_text, n=2)\n",
    "print(f\"Generated prediction: {prediction}\")\n",
    "\n",
    "seed_text = \"daniel ricciardo\"\n",
    "prediction = generate_predictions(trigram_model, seed_text, n=3)\n",
    "print(f\"Generated prediction: {prediction}\")\n",
    "\n",
    "seed_text = \"lewis hamilton\"\n",
    "prediction = generate_predictions(trigram_model, seed_text, n=3)\n",
    "print(f\"Generated prediction: {prediction}\")\n",
    "\n",
    "seed_text = \"max verstappen\"\n",
    "prediction = generate_predictions(trigram_model, seed_text, n=3)\n",
    "print(f\"Generated prediction: {prediction}\")\n",
    "\n",
    "seed_text = \"sergio perez\"\n",
    "prediction = generate_predictions(trigram_model, seed_text, n=3)\n",
    "print(f\"Generated prediction: {prediction}\")\n",
    "\n",
    "seed_text = \"perez to\"\n",
    "prediction = generate_predictions(trigram_model_s, seed_text, n=3)\n",
    "print(f\"Generated prediction: {prediction}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
