{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from config import config\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections.abc import Generator\n",
    "from typing import Any, TypeAlias\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from gensim.utils import tokenize\n",
    "import en_core_web_sm # python3 -m spacy download en_core_web_sm\n",
    "spacy_nlp = en_core_web_sm.load()\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "nltk.download('stopwords', quiet=False)\n",
    "from nltk.metrics.distance import edit_distance\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import words\n",
    "word_list = set(wn.all_lemmas())\n",
    "\n",
    "# Download the words corpus\n",
    "\n",
    "# Access the word list\n",
    "english_vocab = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_FORMULA1_COMMENTS_FILE = config.RAW_DATA_DIR / 'formula1_comments.ndjson'\n",
    "RAW_FORMULA1_SUBMISSIONS_FILE = config.RAW_DATA_DIR / 'formula1_submissions.ndjson'\n",
    "RAW_FORMULA1POINT5_COMMENTS_FILE = config.RAW_DATA_DIR / 'formula1point5_comments.ndjson'\n",
    "RAW_FORMULA1POINT5_SUBMISSIONS_FILE = config.RAW_DATA_DIR / 'formula1point5_submissions.ndjson'\n",
    "\n",
    "CommentsDict: TypeAlias = dict[str, Any]\n",
    "SubmissionsDict: TypeAlias = dict[str, Any]\n",
    "\n",
    "def load_ndjson(ndjson_file: Path) -> Generator[dict[str, Any]]:\n",
    "    with open(ndjson_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            yield json.loads(line)\n",
    "\n",
    "def load_comments(ndjson_file: Path) -> Generator[CommentsDict]:\n",
    "    yield from load_ndjson(ndjson_file)\n",
    "\n",
    "def load_submissions(ndjson_file: Path) -> Generator[SubmissionsDict]:\n",
    "    yield from load_ndjson(ndjson_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tuple(load_submissions(RAW_FORMULA1POINT5_SUBMISSIONS_FILE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formula1_comments = tuple(load_comments(RAW_FORMULA1_COMMENTS_FILE))\n",
    "# formula1_submissions = tuple(load_submissions(RAW_FORMULA1_SUBMISSIONS_FILE))\n",
    "formula1point5_comments = tuple(load_comments(RAW_FORMULA1POINT5_COMMENTS_FILE))\n",
    "formula1point5_submissions = tuple(load_submissions(RAW_FORMULA1POINT5_SUBMISSIONS_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADED_COMMENTS_COLUMNS = (\n",
    "#     'all_awardings',\n",
    "#     'associated_award',\n",
    "#     'author',\n",
    "# )\n",
    "\n",
    "df = pd.DataFrame(load_comments(RAW_FORMULA1POINT5_COMMENTS_FILE))\n",
    "display(df.head())\n",
    "df = df.convert_dtypes()\n",
    "df.dtypes\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_comment = formula1point5_comments[15]['body']\n",
    "print(random_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_newlines(s):\n",
    "    return '\\n'.join(s[:])\n",
    "\n",
    "import re\n",
    "print('|'.join(word_tokenize(random_comment)))\n",
    "print(re.sub(r'\\s+', '|', random_comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'|'.join(tokenize(random_comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('|'.join(random_comment.split()))\n",
    "print('|'.join(token.text for token in spacy_nlp(random_comment)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_names= {\n",
    "    'max verstappen',\n",
    "    'charles leclerc',\n",
    "    'sergio perez',\n",
    "    'george russell',\n",
    "    'carlos sainz',\n",
    "    'lewis hamilton',\n",
    "    'lando norris',\n",
    "    'esteban ocon',\n",
    "    'fernando alonso',\n",
    "    'valtteri bottas',\n",
    "    'daniel ricciardo',\n",
    "    'sebastian vettel',\n",
    "    'kevin magnussen',\n",
    "    'pierre gasly',\n",
    "    'lance stroll',\n",
    "    'mick schumacher',\n",
    "    'yuki tsunoda',\n",
    "    'zhou guanyu',\n",
    "    'alexander albon',\n",
    "    'nicholas latifi',\n",
    "    'nyck de vries',\n",
    "    'nico hulkenberg',\n",
    "    'oscar piastri',\n",
    "    'liam lawson',\n",
    "    'logan sargeant'\n",
    "}\n",
    "\n",
    "F1_DRIVERS = {\n",
    "    'max', 'verstappen',\n",
    "    'charles', 'leclerc',\n",
    "    'sergio', 'perez',\n",
    "    'george', 'russell',\n",
    "    'carlos', 'sainz',\n",
    "    'lewis', 'hamilton',\n",
    "    'lando', 'norris',\n",
    "    'esteban', 'ocon',\n",
    "    'fernando', 'alonso',\n",
    "    'valtteri', 'bottas',\n",
    "    'daniel', 'ricciardo',\n",
    "    'sebastian', 'vettel',\n",
    "    'kevin', 'magnussen',\n",
    "    'pierre', 'gasly',\n",
    "    'lance', 'stroll',\n",
    "    'mick', 'schumacher',\n",
    "    'yuki', 'tsunoda',\n",
    "    'zhou', 'guanyu',\n",
    "    'alexander', 'albon',\n",
    "    'nicholas', 'latifi',\n",
    "    'nyck', 'vries',\n",
    "    'nico', 'hulkenberg',\n",
    "    'oscar', 'piastri',\n",
    "    'liam', 'lawson',\n",
    "    'logan', 'sargeant'\n",
    "}\n",
    "\n",
    "Drivers_dict = {\n",
    "    'max': 'max verstappen',\n",
    "    'charles': 'charles leclerc',\n",
    "    'sergio': 'sergio perez',\n",
    "    'george': 'george russell',\n",
    "    'carlos': 'carlos sainz',\n",
    "    'lewis': 'lewis hamilton',\n",
    "    'lando': 'lando norris',\n",
    "    'esteban': 'esteban ocon',\n",
    "    'fernando': 'fernando alonso',\n",
    "    'valtteri': 'valtteri bottas',\n",
    "    'daniel': 'daniel ricciardo',\n",
    "    'sebastian': 'sebastian vettel',\n",
    "    'kevin': 'kevin magnussen',\n",
    "    'pierre': 'pierre gasly',\n",
    "    'lance': 'lance stroll',\n",
    "    'mick': 'mick schumacher',\n",
    "    'yuki': 'yuki tsunoda',\n",
    "    'zhou': 'zhou guanyu',\n",
    "    'alexander': 'alexander albon',\n",
    "    'nicholas': 'nicholas latifi',\n",
    "    'nyck': 'nyck de vries',\n",
    "    'nico': 'nico hulkenberg',\n",
    "    'oscar': 'oscar piastri',\n",
    "    'liam': 'liam lawson',\n",
    "    'logan': 'logan sargeant',\n",
    "    \n",
    "    'verstappen': 'max verstappen',\n",
    "    'leclerc': 'charles leclerc',\n",
    "    'perez': 'sergio perez',\n",
    "    'russell': 'george russell',\n",
    "    'sainz': 'carlos sainz',\n",
    "    'hamilton': 'lewis hamilton',\n",
    "    'norris': 'lando norris',\n",
    "    'ocon': 'esteban ocon',\n",
    "    'alonso': 'fernando alonso',\n",
    "    'bottas': 'valtteri bottas',\n",
    "    'ricciardo': 'daniel ricciardo',\n",
    "    'vettel': 'sebastian vettel',\n",
    "    'magnussen': 'kevin magnussen',\n",
    "    'gasly': 'pierre gasly',\n",
    "    'stroll': 'lance stroll',\n",
    "    'schumacher': 'mick schumacher',\n",
    "    'tsunoda': 'yuki tsunoda',\n",
    "    'guanyu': 'zhou guanyu',\n",
    "    'albon': 'alexander albon',\n",
    "    'latifi': 'nicholas latifi',\n",
    "    'vries': 'nyck de vries',\n",
    "    'hulkenberg': 'nico hulkenberg',\n",
    "    'piastri': 'oscar piastri',\n",
    "    'lawson': 'liam lawson',\n",
    "    'sargean': 'logan sargeant',\n",
    "}\n",
    "\n",
    "F1_DRIVERS = {\n",
    "    'max', 'verstappen',\n",
    "    'charles', 'leclerc',\n",
    "    'sergio', 'perez',\n",
    "    'george', 'russell',\n",
    "    'carlos', 'sainz',\n",
    "    'lewis', 'hamilton',\n",
    "    'lando', 'norris',\n",
    "    'esteban', 'ocon',\n",
    "    'fernando', 'alonso',\n",
    "    'valtteri', 'bottas',\n",
    "    'daniel', 'ricciardo',\n",
    "    'sebastian', 'vettel',\n",
    "    'kevin', 'magnussen',\n",
    "    'pierre', 'gasly',\n",
    "    'lance', 'stroll',\n",
    "    'mick', 'schumacher',\n",
    "    'yuki', 'tsunoda',\n",
    "    'zhou', 'guanyu',\n",
    "    'alexander', 'albon',\n",
    "    'nicholas', 'latifi',\n",
    "    'nyck', 'vries',\n",
    "    'nico', 'hulkenberg',\n",
    "    'oscar', 'piastri',\n",
    "    'liam', 'lawson',\n",
    "    'logan', 'sargeant'\n",
    "}\n",
    "\n",
    "F1_VOCABULARY = F1_DRIVERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(comment):\n",
    "    comment = comment.lower()\n",
    "    comment = re.sub(r'http\\S+', '', comment)\n",
    "    comment = re.sub(r'[^a-z\\s\\d]', '', comment)\n",
    "    return comment\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = []\n",
    "    for token in spacy_nlp(text):\n",
    "        tokens.append(token.text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def remove_stopword(tokens, stop_words=None):\n",
    "    if stop_words is None:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    new_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:\n",
    "            new_tokens.append(word)\n",
    "    return new_tokens\n",
    "\n",
    "\n",
    "def lemmatize(tokens):\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return lemmatized_tokens\n",
    "\n",
    "\n",
    "def NER(tokens):\n",
    "    s = ' '.join(tokens)\n",
    "\n",
    "    doc = spacy_nlp(s)\n",
    "\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "def correct_spelling(word):\n",
    "    if word in F1_VOCABULARY:\n",
    "        return word\n",
    "    \n",
    "    min_distance = float('inf')\n",
    "    corrected_word = word\n",
    "    for term in F1_VOCABULARY:\n",
    "        distance = edit_distance(word, term)\n",
    "        if distance < min_distance and distance <= max(1, len(word)//3):  # Allow a maximum edit distance of 33%\n",
    "            min_distance = distance\n",
    "            corrected_word = term\n",
    "    # print(word, corrected_word, min_distance)\n",
    "    return corrected_word\n",
    "\n",
    "def combine_names(tokens):\n",
    "    combined_tokens = []\n",
    "    skip_next = False\n",
    "\n",
    "    for i, word in enumerate(tokens):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "\n",
    "        # Check if the current word and the next word form a driver name\n",
    "        if i + 1 < len(tokens):\n",
    "            combined_word = f'{word} {tokens[i + 1]}'\n",
    "            if combined_word in F1_names:\n",
    "                combined_tokens.append(combined_word)\n",
    "                skip_next = True\n",
    "                continue\n",
    "\n",
    "        # Check if the current word alone matches a driver name\n",
    "        if word in F1_DRIVERS:\n",
    "            combined_tokens.append(Drivers_dict[word])\n",
    "        else:\n",
    "            combined_tokens.append(word)\n",
    "\n",
    "    return combined_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = 'i am really not a big fan of msx verstappening i am much more a fan of charles lecllerf fuck maxipad verstappen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to perform PoS tagging using NLTK\n",
    "# INPUT: Raw text\n",
    "# Output: A list of tuples including each word and its respective PoS tag\n",
    "def pos_tagging_nltk(text):\n",
    "    \n",
    "    # first we tokenize the raw text\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # extract the PoS tags\n",
    "    pos_tags = nltk.pos_tag(tokenized_text)\n",
    "    \n",
    "    # return the PoS tags\n",
    "    return pos_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comment in ()r comment i\n",
    "comment = random_comment\n",
    "comment = 'I am really not a big fan of msx verstappening, i am much more a fan of charles lecllerf'\n",
    "\n",
    "\n",
    "normalized = normalize(comment)\n",
    "tokens = tokenize(normalized)\n",
    "\n",
    "corrected = []\n",
    "for word in tokens:\n",
    "    corrected.append(correct_spelling(word))\n",
    "\n",
    "combined = combine_names(corrected)\n",
    "lemmatized = lemmatize(combined)\n",
    "no_stopwords = remove_stopword(lemmatized)\n",
    "named_entities = NER(comment)\n",
    "\n",
    "print(comment)\n",
    "print(combined)\n",
    "print(lemmatized)\n",
    "print(no_stopwords)\n",
    "print(named_entities)\n",
    "# print(pos_tagging_nltk(' '.join(lemmatized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levenshtein_distance('max verstappen', 'max verstappening')\n",
    "word = correct_spelling(\"max verstappening\")\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
