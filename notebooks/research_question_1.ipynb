{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Generator, Callable\n",
    "from pathlib import Path\n",
    "import typing\n",
    "from typing import Any, TypeAlias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import re\n",
    "from functools import partial, reduce\n",
    "from tqdm import tqdm\n",
    "from IPython.display import (\n",
    "    display, # type: ignore[reportUnknownVariableType]\n",
    "    Markdown,\n",
    ")\n",
    "\n",
    "import importlib\n",
    "\n",
    "from config.fastf1 import fastf1\n",
    "from config import config\n",
    "import src.data.constants as dataset_constants\n",
    "importlib.reload(dataset_constants);\n",
    "import src.data.loader\n",
    "importlib.reload(src.data.loader);\n",
    "from src.data.loader import stream_ndjson, load_submissions_df, load_comments_df\n",
    "from src.data.preprocessing import concatenate_submissions_and_comments\n",
    "\n",
    "from src.utils import (\n",
    "    temporary_pandas_options,\n",
    "    display_full_dataframe,\n",
    "    hide_index,\n",
    "    compose,\n",
    ")\n",
    "from src import utils\n",
    "utils.set_random_seeds()\n",
    "DEVICE = utils.get_device()\n",
    "\n",
    "import logging\n",
    "logging.getLogger('fastf1').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and find submissions related to steward decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_submissions_df = load_submissions_df(\n",
    "    dataset_constants.RawFile.FORMULA1_SUBMISSIONS,\n",
    "    columns=dataset_constants.DEFAULT_SUBMISSION_COLUMNS | {'permalink', 'post_hint', 'link_flair_text'},\n",
    ")                                  \n",
    "\n",
    "f1_comments_df = load_comments_df(\n",
    "    dataset_constants.RawFile.FORMULA1_COMMENTS,\n",
    "    columns=dataset_constants.DEFAULT_COMMENT_COLUMNS | {'link_id'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_comments_df = f1_comments_df[~f1_comments_df['body'].isin({'[removed]', '[deleted]'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: testing purposes\n",
    "f1_submissions_df['permalink'] = 'www.reddit.com' + f1_submissions_df['permalink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steward_decision_related_words = {\n",
    "    'penalty', 'steward', 'decision', 'appeal', 'review', 'ruling', 'investigation', 'regulation',\n",
    "    'seconds', 'sec', \n",
    "    'collision', 'crash', 'incident', 'overtake', 'virtual safety car', 'blocking', 'brake test', 'contact',\n",
    "    'red flag', 'yellow flag', \n",
    "    'controversial', 'rigged', 'corrupt', 'bias', 'protest', 'FIA', 'document', 'infringement'}\n",
    "\n",
    "# Manually exclude some posts unrelated to steward decisions\n",
    "excluded_submission_ids = {\n",
    "    'vdr1c6',\n",
    "    'w7z5aj',\n",
    "    'wf87e0',\n",
    "    'x1zd5z',\n",
    "    'x3y140',\n",
    "}\n",
    "\n",
    "words_regex = ''.join(fr'\\b{word}\\b|' for word in steward_decision_related_words)[:-1]\n",
    "steward_decision_pattern = re.compile(words_regex, flags=re.IGNORECASE)\n",
    "\n",
    "relevant_flairs = {':post-technical: Technical', ':post-news: News'}\n",
    "\n",
    "has_related_words = f1_submissions_df['title'].apply(lambda title: steward_decision_pattern.search(title) is not None)\n",
    "has_relevant_flairs = f1_submissions_df['link_flair_text'].isin(relevant_flairs)\n",
    "is_image_post = f1_submissions_df['post_hint'] == 'image'\n",
    "is_included = ~f1_submissions_df['id'].isin(excluded_submission_ids) \n",
    "\n",
    "steward_decision_submissions_df = f1_submissions_df[has_related_words & has_relevant_flairs & is_image_post & is_included].copy()\n",
    "\n",
    "with display_full_dataframe():\n",
    "    print(len(steward_decision_submissions_df))\n",
    "    display(steward_decision_submissions_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_link_id = f't3_xtqa50'\n",
    "comments_df = f1_comments_df[f1_comments_df['link_id'] == submission_link_id]\n",
    "print(len(comments_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretization of continuous sentiment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentiment_category(sentiment: float) -> str:\n",
    "    if sentiment >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif sentiment <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: testing\n",
    "steward_decision_submissions_df = load_submissions_df(dataset_constants.RawFile.FORMULA1_SUBMISSIONS, partial(stream_ndjson, limit=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_vader_sentiments = []\n",
    "\n",
    "for index, steward_decision_submission in steward_decision_submissions_df.iterrows():\n",
    "    submission_link_id = f't3_{steward_decision_submission['id']}'\n",
    "    comments_df = f1_comments_df[f1_comments_df['link_id'] == submission_link_id].copy()\n",
    "\n",
    "    if comments_df.empty:\n",
    "        steward_decision_submissions_df.loc[index, 'average_sentiment_vader'] = np.nan\n",
    "        continue\n",
    "    \n",
    "    NOEMER = np.abs(comments_df['score']).sum()\n",
    "    \n",
    "    if NOEMER == 0:\n",
    "        steward_decision_submissions_df.loc[index, 'average_sentiment_vader'] = np.nan\n",
    "        continue\n",
    "\n",
    "    comments_df.loc[:, 'compound'] = comments_df['body'].apply(\n",
    "        lambda text: analyzer.polarity_scores(text)['compound']\n",
    "    )   #creates a data series with a score (float) given an input comment\n",
    "\n",
    "    sentiment = (comments_df['compound'] * comments_df['score']).sum() / NOEMER\n",
    "    steward_decision_submissions_df.loc[index, 'average_sentiment_vader'] = sentiment\n",
    "\n",
    "    individual_vader_sentiments.append(sentiment)\n",
    "\n",
    "# print(individual_vader_sentiments[:10])\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(steward_decision_submissions_df.head(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TOOD: vectorize if efficiency is needed\n",
    "# steward_decision_submissions_df['submission_link_id'] = 't3_' + steward_decision_submissions_df['id']\n",
    "\n",
    "# filtered_comments_df = f1_comments_df[\n",
    "#     f1_comments_df['link_id'].isin(steward_decision_submissions_df['submission_link_id'])\n",
    "# ].copy()\n",
    "\n",
    "# filtered_comments_df['compound'] = filtered_comments_df['body'].apply(\n",
    "#     lambda text: analyzer.polarity_scores(text)['compound']\n",
    "# )\n",
    "\n",
    "# def calculate_weighted_sentiment(group):\n",
    "#     NOEMER = np.abs(group['score']).sum()\n",
    "#     if NOEMER == 0:\n",
    "#         return np.nan\n",
    "#     return (group['compound'] * group['score']).sum() / NOEMER\n",
    "\n",
    "# average_sentiment = filtered_comments_df.groupby('link_id').apply(calculate_weighted_sentiment)\n",
    "\n",
    "# steward_decision_submissions_df['average_sentiment_vader'] = \\\n",
    "#     steward_decision_submissions_df['submission_link_id'].map(average_sentiment)\n",
    "\n",
    "# steward_decision_submissions_df.drop(columns=['submission_link_id'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "def BERT_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    sentiment_score = torch.argmax(logits, dim=1).item()  # 0: negative, 1: neutral, 2: positive\n",
    "    return (sentiment_score - 1) #-1: negative, 0: neutral, 1: positive\n",
    "\n",
    "individual_bert_sentiments = []\n",
    "\n",
    "for index, steward_decision_submission in steward_decision_submissions_df.iterrows():\n",
    "    submission_link_id = f't3_{steward_decision_submission['id']}'\n",
    "    comments_df = f1_comments_df[f1_comments_df['link_id'] == submission_link_id].copy()\n",
    "\n",
    "    if comments_df.empty:\n",
    "        steward_decision_submissions_df.loc[index, 'average_sentiment_bert'] = np.nan\n",
    "        individual_bert_sentiments.append(sentiment)  # Store NaN in the list\n",
    "        continue\n",
    "    \n",
    "    NOEMER = np.abs(comments_df['score']).sum()\n",
    "    \n",
    "    if NOEMER == 0:\n",
    "        steward_decision_submissions_df.loc[index, 'average_sentiment_bert'] = np.nan\n",
    "        individual_bert_sentiments.append(sentiment)  # Store NaN in the list\n",
    "        continue\n",
    "\n",
    "    comments_df.loc[:, 'compound'] = comments_df['body'].apply(BERT_sentiment)\n",
    "        # lambda text: analyzer.polarity_scores(text)['compound']\n",
    "        # bert induced sentiment\n",
    "\n",
    "    sentiment = (comments_df['compound'] * comments_df['score']).sum() / NOEMER\n",
    "    steward_decision_submissions_df.loc[index, 'average_sentiment_bert'] = sentiment\n",
    "\n",
    "    individual_bert_sentiments.append(sentiment)  # Append BERT sentiment to list\n",
    "\n",
    "#  with display_full_dataframe():\n",
    "    # display(steward_decision_submissions_df)\n",
    "    # display(steward_decision_submissions_df['average_sentiment_bert'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with display_full_dataframe():\n",
    "    display(steward_decision_submissions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN ABSOLUTE ERROR\n",
    "print(\n",
    "    np.abs(steward_decision_submissions_df['average_sentiment_bert'] - steward_decision_submissions_df['average_sentiment_vader']).sum() \\\n",
    "    / len(steward_decision_submissions_df.index)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(steward_decision_submissions_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALIDATION\n",
    "# 1. CATEGORIZATIONI AGREEMENT METRIC - Cohen's Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Vader_sentiments = steward_decision_submissions_df['average_sentiment_vader']\n",
    "# Bert_sentiments = steward_decision_submissions_df['average_sentiment_bert']\n",
    "\n",
    "vader_sentiment_labels = np.array([to_sentiment_category(x) for x in individual_vader_sentiments])\n",
    "bert_sentiment_labels = np.array([to_sentiment_category(y) for y in individual_bert_sentiments])\n",
    "\n",
    "COHEN_KAPPA = cohen_kappa_score(vader_sentiment_labels, bert_sentiment_labels)\n",
    "print(f\"Cohen's Kappa: {COHEN_KAPPA:.2f}\")\n",
    "\n",
    "# Kappa > 0.75 is a Strong agreement\n",
    "# Kappa = 0.4 - 0.75 is a Moderate agreement\n",
    "# Kappa < 0.4 is a Weak agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. BIAS DETECTION METRIC - Bland-Altman plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate systematic biases between VADER and BERT sentiment analysis\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "mean_scores = (np.array(individual_vader_sentiments) + np.array(individual_bert_sentiments)) / 2\n",
    "diff_scores = np.array(individual_vader_sentiments) - np.array(individual_bert_sentiments)\n",
    "\n",
    "# Create a scatter plot for the Bland-Altman analysis\n",
    "sns.scatterplot(x=mean_scores, y=diff_scores)\n",
    "plt.axhline(0, color='red', linestyle='dashed')  # No bias line\n",
    "plt.xlabel(\"Mean Sentiment Score\")\n",
    "plt.ylabel(\"VADER - BERT Sentiment Score Difference\")\n",
    "plt.title(\"Bland-Altman Plot\")\n",
    "plt.show()\n",
    "# patterns indicate bias\n",
    "# if most points are close to 0, the models mostly agree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
