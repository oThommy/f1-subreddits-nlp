{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Generator, Callable\n",
    "from pathlib import Path\n",
    "import typing\n",
    "from typing import Any, TypeAlias, Literal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import re\n",
    "from functools import partial, reduce\n",
    "from tqdm import tqdm\n",
    "from IPython.display import (\n",
    "    display, # type: ignore[reportUnknownVariableType]\n",
    "    Markdown,\n",
    ")\n",
    "\n",
    "import importlib\n",
    "\n",
    "from config.fastf1 import fastf1\n",
    "from config import config\n",
    "import src.data.constants as dataset_constants\n",
    "importlib.reload(dataset_constants);\n",
    "import src.data.loader\n",
    "importlib.reload(src.data.loader);\n",
    "from src.data.loader import stream_ndjson, load_submissions_df, load_comments_df\n",
    "from src.data.preprocessing import concatenate_submissions_and_comments\n",
    "\n",
    "from src.utils import (\n",
    "    temporary_pandas_options,\n",
    "    display_full_dataframe,\n",
    "    hide_index,\n",
    "    compose,\n",
    ")\n",
    "from src import utils\n",
    "utils.set_random_seeds()\n",
    "DEVICE = utils.get_device()\n",
    "\n",
    "import logging\n",
    "logging.getLogger('fastf1').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and find submissions related to steward decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_submissions_df = load_submissions_df(\n",
    "    dataset_constants.RawFile.FORMULA1_SUBMISSIONS,\n",
    "    columns=dataset_constants.DEFAULT_SUBMISSION_COLUMNS | {'permalink', 'post_hint', 'link_flair_text'},\n",
    ")                                  \n",
    "\n",
    "f1_comments_df = load_comments_df(\n",
    "    dataset_constants.RawFile.FORMULA1_COMMENTS,\n",
    "    columns=dataset_constants.DEFAULT_COMMENT_COLUMNS | {'link_id'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_comments_df = f1_comments_df[~f1_comments_df['body'].isin({'[removed]', '[deleted]'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_submissions_df['permalink'] = 'www.reddit.com' + f1_submissions_df['permalink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steward_decision_related_words = {\n",
    "    'penalty', 'steward', 'decision', 'appeal', 'review', 'ruling', 'investigation', 'regulation',\n",
    "    'seconds', 'sec', \n",
    "    'collision', 'crash', 'incident', 'overtake', 'virtual safety car', 'blocking', 'brake test', 'contact',\n",
    "    'red flag', 'yellow flag', \n",
    "    'controversial', 'rigged', 'corrupt', 'bias', 'protest', 'FIA', 'document', 'infringement'}\n",
    "\n",
    "# Manually exclude some posts unrelated to steward decisions\n",
    "excluded_submission_ids = {\n",
    "    'vdr1c6',\n",
    "    'w7z5aj',\n",
    "    'wf87e0',\n",
    "    'x1zd5z',\n",
    "    'x3y140',\n",
    "}\n",
    "\n",
    "words_regex = ''.join(fr'\\b{word}\\b|' for word in steward_decision_related_words)[:-1]\n",
    "steward_decision_pattern = re.compile(words_regex, flags=re.IGNORECASE)\n",
    "\n",
    "relevant_flairs = {':post-technical: Technical', ':post-news: News'}\n",
    "\n",
    "has_related_words = f1_submissions_df['title'].apply(lambda title: steward_decision_pattern.search(title) is not None)\n",
    "has_relevant_flairs = f1_submissions_df['link_flair_text'].isin(relevant_flairs)\n",
    "is_image_post = f1_submissions_df['post_hint'] == 'image'\n",
    "is_included = ~f1_submissions_df['id'].isin(excluded_submission_ids) \n",
    "\n",
    "steward_decision_submissions_df = f1_submissions_df[has_related_words & has_relevant_flairs & is_image_post & is_included].copy()\n",
    "\n",
    "with display_full_dataframe():\n",
    "    print(len(steward_decision_submissions_df))\n",
    "    display(steward_decision_submissions_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretization of continuous sentiment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_range = (-0.05, 0.05)\n",
    "\n",
    "def to_sentiment_category(sentiment: float) -> Literal['Positive', 'Negative', 'Neutral']:\n",
    "    if sentiment >= neutral_range[1]:\n",
    "        return 'Positive'\n",
    "    elif sentiment <= -neutral_range[0]:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "def to_discrete_sentiment(sentiment: float) -> int:\n",
    "    category = to_sentiment_category(sentiment)\n",
    "\n",
    "    match category:\n",
    "        case 'Positive':\n",
    "            return 1\n",
    "        case 'Negative':\n",
    "            return -1\n",
    "        case 'Neutral':\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "vader_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, steward_decision_submission in steward_decision_submissions_df.iterrows():\n",
    "    submission_link_id = f't3_{steward_decision_submission['id']}'\n",
    "    comments_df = f1_comments_df[f1_comments_df['link_id'] == submission_link_id].copy()\n",
    "\n",
    "    if comments_df.empty:\n",
    "        steward_decision_submissions_df.loc[index, 'average_sentiment_vader'] = np.nan\n",
    "        continue\n",
    "    \n",
    "    number_of_votes = np.abs(comments_df['score']).sum()\n",
    "    \n",
    "    if number_of_votes == 0:\n",
    "        steward_decision_submissions_df.loc[index, 'average_sentiment_vader'] = np.nan\n",
    "        continue\n",
    "\n",
    "    comments_df.loc[:, 'compound'] = comments_df['body'].apply(\n",
    "        lambda text: vader_analyzer.polarity_scores(text)['compound']\n",
    "    )\n",
    "\n",
    "    average_sentiment = (comments_df['compound'] * comments_df['score']).sum() / number_of_votes\n",
    "    steward_decision_submissions_df.loc[index, 'average_sentiment_vader'] = average_sentiment\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(steward_decision_submissions_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model.to(DEVICE);\n",
    "\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "def bert_sentiment(text: str) -> float:\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    sentiment_score = torch.argmax(logits, dim=1).item()  # 0: negative, 1: neutral, 2: positive\n",
    "\n",
    "    return (sentiment_score - 1) #-1: negative, 0: neutral, 1: positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, steward_decision_submission in steward_decision_submissions_df.iterrows():\n",
    "    submission_link_id = f't3_{steward_decision_submission['id']}'\n",
    "    comments_df = f1_comments_df[f1_comments_df['link_id'] == submission_link_id].copy()\n",
    "\n",
    "    if comments_df.empty:\n",
    "        steward_decision_submissions_df.loc[index, 'average_sentiment_bert'] = np.nan\n",
    "        continue\n",
    "    \n",
    "    number_of_votes = np.abs(comments_df['score']).sum()\n",
    "    \n",
    "    if number_of_votes == 0:\n",
    "        steward_decision_submissions_df.loc[index, 'average_sentiment_bert'] = np.nan\n",
    "        continue\n",
    "\n",
    "    comments_df.loc[:, 'compound'] = comments_df['body'].apply(bert_sentiment)\n",
    "\n",
    "    average_sentiment = (comments_df['compound'] * comments_df['score']).sum() / number_of_votes\n",
    "    steward_decision_submissions_df.loc[index, 'average_sentiment_bert'] = average_sentiment\n",
    "\n",
    "#  with display_full_dataframe():\n",
    "    # display(steward_decision_submissions_df)\n",
    "    # display(steward_decision_submissions_df['average_sentiment_bert'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (extra) sentiment analysis between controversial drivers, \n",
    "# Verstappen & Alonso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verstappen_decisions = steward_decision_submissions_df[\n",
    "    steward_decision_submissions_df['title'].apply(lambda title: re.search(r'max|verstappen', title, flags=re.IGNORECASE) is not None)\n",
    "]\n",
    "alonso_decisions = steward_decision_submissions_df[\n",
    "    steward_decision_submissions_df['title'].apply(lambda title: re.search(r'fernando|alonso', title, flags=re.IGNORECASE) is not None)\n",
    "]\n",
    "\n",
    "ignored_alonso_submissions = {'wzw8e1', 'ybswfs'}\n",
    "alonso_decisions = alonso_decisions[~alonso_decisions['id'].isin(ignored_alonso_submissions)]\n",
    "\n",
    "print(verstappen_decisions['average_sentiment_bert'].mean())\n",
    "print(alonso_decisions['average_sentiment_bert'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(data=steward_decision_submissions_df, x='created_utc', y='average_sentiment_bert', marker='o')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Average Sentiment (BERT)', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRINSIC VALIDATION - CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "validation_labels_dir = config.ROOT_DIR / 'validation_labels' / 'steward_decision_submissions'\n",
    "labeled_comments = tuple(\n",
    "    labeled_comment\n",
    "    for file in validation_labels_dir.glob('*.ndjson')\n",
    "    for labeled_comment in stream_ndjson(file)\n",
    ")\n",
    "\n",
    "comment_ids = {labeled_comment['comment_id'] for labeled_comment in labeled_comments}\n",
    "\n",
    "y_true = [labeled_comment['sentiment'] for labeled_comment in labeled_comments]\n",
    "y_vader = [to_sentiment_category(vader_analyzer.polarity_scores(f1_comments_df.loc[f1_comments_df['id'] == labeled_comments, 'body'].iloc[0])['compound']) for labeled_comment in labeled_comments]\n",
    "y_bert = [to_sentiment_category(bert_sentiment(f1_comments_df.loc[f1_comments_df['id'] == labeled_comment['comment_id'], 'body'].iloc[0])) for labeled_comment in labeled_comments]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for ax, (y_pred, title) in zip(axes, [(y_vader, \"VADER\"), (y_bert, \"BERT\")]):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[\"Positive\", \"Neutral\", \"Negative\"])\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"Positive\", \"Neutral\", \"Negative\"],\n",
    "                yticklabels=[\"Positive\", \"Neutral\", \"Negative\"], ax=ax)\n",
    "    ax.set_title(f\"Confusion Matrix: {title}\")\n",
    "    ax.set_xlabel(\"Predicted Label\")\n",
    "    ax.set_ylabel(\"True Label\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accuracy, precision, recall and F1 validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_vader)\n",
    "print(f\"VADER Accuracy: {accuracy:.4f}\")\n",
    "accuracy = accuracy_score(y_true, y_bert)\n",
    "print(f\"BERT Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "labels = [\"Positive\", \"Neutral\", \"Negative\"]\n",
    "for label in labels:\n",
    "    vader_precision = precision_score(y_true, y_vader, labels=[label], average=\"macro\", zero_division=0)\n",
    "    vader_recall = recall_score(y_true, y_vader, labels=[label], average=\"macro\", zero_division=0)\n",
    "    vader_f1 = f1_score(y_true, y_vader, labels=[label], average=\"macro\", zero_division=0)\n",
    "\n",
    "    bert_precision = precision_score(y_true, y_bert, labels=[label], average=\"macro\", zero_division=0)\n",
    "    bert_recall = recall_score(y_true, y_bert, labels=[label], average=\"macro\", zero_division=0)\n",
    "    bert_f1 = f1_score(y_true, y_bert, labels=[label], average=\"macro\", zero_division=0)\n",
    "\n",
    "    print(f\"\\nMetrics for class '{label}':\")\n",
    "    print(f\"  VADER -> Precision: {vader_precision:.4f}, Recall: {vader_recall:.4f}, F1-score: {vader_f1:.4f}\")\n",
    "    print(f\"  BERT  -> Precision: {bert_precision:.4f}, Recall: {bert_recall:.4f}, F1-score: {bert_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{steward_decision_submissions_df['average_sentiment_bert'].mean()=}')\n",
    "print(f'{steward_decision_submissions_df['average_sentiment_bert'].min()=}')\n",
    "print(f'{steward_decision_submissions_df['average_sentiment_bert'].max()=}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
