{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Generator, Callable\n",
    "from pathlib import Path\n",
    "import typing\n",
    "from typing import Any, TypeAlias, Literal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import re\n",
    "from functools import partial, reduce\n",
    "from tqdm import tqdm\n",
    "from IPython.display import (\n",
    "    display, # type: ignore[reportUnknownVariableType]\n",
    "    Markdown,\n",
    ")\n",
    "\n",
    "import importlib\n",
    "\n",
    "from config.fastf1 import fastf1\n",
    "from config import config\n",
    "import src.data.constants as dataset_constants\n",
    "importlib.reload(dataset_constants);\n",
    "import src.data.loader\n",
    "importlib.reload(src.data.loader);\n",
    "from src.data.loader import stream_ndjson, load_submissions_df, load_comments_df\n",
    "from src.data.preprocessing import concatenate_submissions_and_comments\n",
    "\n",
    "from src.utils import (\n",
    "    temporary_pandas_options,\n",
    "    display_full_dataframe,\n",
    "    hide_index,\n",
    "    compose,\n",
    ")\n",
    "from src import utils\n",
    "utils.set_random_seeds()\n",
    "DEVICE = utils.get_device()\n",
    "\n",
    "import logging\n",
    "logging.getLogger('fastf1').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and find submissions related to steward decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_submissions_df = load_submissions_df(\n",
    "    dataset_constants.RawFile.FORMULA1_SUBMISSIONS,\n",
    "    columns=dataset_constants.DEFAULT_SUBMISSION_COLUMNS | {'permalink', 'post_hint', 'link_flair_text'},\n",
    ")                                  \n",
    "\n",
    "f1_comments_df = load_comments_df(\n",
    "    dataset_constants.RawFile.FORMULA1_COMMENTS,\n",
    "    columns=dataset_constants.DEFAULT_COMMENT_COLUMNS | {'link_id'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_comments_df = f1_comments_df[~f1_comments_df['body'].isin({'[removed]', '[deleted]'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: testing purposes\n",
    "f1_submissions_df['permalink'] = 'www.reddit.com' + f1_submissions_df['permalink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steward_decision_related_words = {\n",
    "    'penalty', 'steward', 'decision', 'appeal', 'review', 'ruling', 'investigation', 'regulation',\n",
    "    'seconds', 'sec', \n",
    "    'collision', 'crash', 'incident', 'overtake', 'virtual safety car', 'blocking', 'brake test', 'contact',\n",
    "    'red flag', 'yellow flag', \n",
    "    'controversial', 'rigged', 'corrupt', 'bias', 'protest', 'FIA', 'document', 'infringement'}\n",
    "\n",
    "# Manually exclude some posts unrelated to steward decisions\n",
    "excluded_submission_ids = {\n",
    "    'vdr1c6',\n",
    "    'w7z5aj',\n",
    "    'wf87e0',\n",
    "    'x1zd5z',\n",
    "    'x3y140',\n",
    "}\n",
    "\n",
    "words_regex = ''.join(fr'\\b{word}\\b|' for word in steward_decision_related_words)[:-1]\n",
    "steward_decision_pattern = re.compile(words_regex, flags=re.IGNORECASE)\n",
    "\n",
    "relevant_flairs = {':post-technical: Technical', ':post-news: News'}\n",
    "\n",
    "has_related_words = f1_submissions_df['title'].apply(lambda title: steward_decision_pattern.search(title) is not None)\n",
    "has_relevant_flairs = f1_submissions_df['link_flair_text'].isin(relevant_flairs)\n",
    "is_image_post = f1_submissions_df['post_hint'] == 'image'\n",
    "is_included = ~f1_submissions_df['id'].isin(excluded_submission_ids) \n",
    "\n",
    "steward_decision_submissions_df = f1_submissions_df[has_related_words & has_relevant_flairs & is_image_post & is_included].copy()\n",
    "\n",
    "with display_full_dataframe():\n",
    "    print(len(steward_decision_submissions_df))\n",
    "    display(steward_decision_submissions_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretization of continuous sentiment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_range = (-0.05, 0.05)\n",
    "\n",
    "def to_sentiment_category(sentiment: float) -> Literal['Positive', 'Negative', 'Neutral']:\n",
    "    if sentiment >= neutral_range[1]:\n",
    "        return 'Positive'\n",
    "    elif sentiment <= -neutral_range[0]:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "def to_discrete_sentiment(sentiment: float) -> int:\n",
    "    category = to_sentiment_category(sentiment)\n",
    "\n",
    "    match category:\n",
    "        case 'Positive':\n",
    "            return 1\n",
    "        case 'Negative':\n",
    "            return -1\n",
    "        case 'Neutral':\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "vader_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, steward_decision_submission in steward_decision_submissions_df.iterrows():\n",
    "    submission_link_id = f't3_{steward_decision_submission['id']}'\n",
    "    comments_df = f1_comments_df[f1_comments_df['link_id'] == submission_link_id].copy()\n",
    "\n",
    "    if comments_df.empty:\n",
    "        steward_decision_submissions_df.loc[index, 'average_sentiment_vader'] = np.nan\n",
    "        continue\n",
    "    \n",
    "    number_of_votes = np.abs(comments_df['score']).sum()\n",
    "    \n",
    "    if number_of_votes == 0:\n",
    "        steward_decision_submissions_df.loc[index, 'average_sentiment_vader'] = np.nan\n",
    "        continue\n",
    "\n",
    "    comments_df.loc[:, 'compound'] = comments_df['body'].apply(\n",
    "        lambda text: vader_analyzer.polarity_scores(text)['compound']\n",
    "    )\n",
    "\n",
    "    average_sentiment = (comments_df['compound'] * comments_df['score']).sum() / number_of_votes\n",
    "    steward_decision_submissions_df.loc[index, 'average_sentiment_vader'] = average_sentiment\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(steward_decision_submissions_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model.to(DEVICE);\n",
    "\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "def bert_sentiment(text: str) -> float:\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    sentiment_score = torch.argmax(logits, dim=1).item()  # 0: negative, 1: neutral, 2: positive\n",
    "\n",
    "    return (sentiment_score - 1) #-1: negative, 0: neutral, 1: positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, steward_decision_submission in steward_decision_submissions_df.iterrows():\n",
    "    submission_link_id = f't3_{steward_decision_submission['id']}'\n",
    "    comments_df = f1_comments_df[f1_comments_df['link_id'] == submission_link_id].copy()\n",
    "\n",
    "    if comments_df.empty:\n",
    "        steward_decision_submissions_df.loc[index, 'average_sentiment_bert'] = np.nan\n",
    "        continue\n",
    "    \n",
    "    number_of_votes = np.abs(comments_df['score']).sum()\n",
    "    \n",
    "    if number_of_votes == 0:\n",
    "        steward_decision_submissions_df.loc[index, 'average_sentiment_bert'] = np.nan\n",
    "        continue\n",
    "\n",
    "    comments_df.loc[:, 'compound'] = comments_df['body'].apply(bert_sentiment)\n",
    "\n",
    "    average_sentiment = (comments_df['compound'] * comments_df['score']).sum() / number_of_votes\n",
    "    steward_decision_submissions_df.loc[index, 'average_sentiment_bert'] = average_sentiment\n",
    "\n",
    "#  with display_full_dataframe():\n",
    "    # display(steward_decision_submissions_df)\n",
    "    # display(steward_decision_submissions_df['average_sentiment_bert'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with display_full_dataframe():\n",
    "    display(steward_decision_submissions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN ABSOLUTE ERROR\n",
    "print(\n",
    "    np.abs(steward_decision_submissions_df['average_sentiment_bert'] - steward_decision_submissions_df['average_sentiment_vader']).sum() \\\n",
    "    / len(steward_decision_submissions_df.index)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(steward_decision_submissions_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALIDATION\n",
    "# 1. CATEGORIZATIONI AGREEMENT METRIC - Cohen's Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Vader_sentiments = steward_decision_submissions_df['average_sentiment_vader']\n",
    "# Bert_sentiments = steward_decision_submissions_df['average_sentiment_bert']\n",
    "\n",
    "vader_sentiment_labels = np.array([to_sentiment_category(x) for x in individual_vader_sentiments])\n",
    "bert_sentiment_labels = np.array([to_sentiment_category(y) for y in individual_bert_sentiments])\n",
    "\n",
    "COHEN_KAPPA = cohen_kappa_score(vader_sentiment_labels, bert_sentiment_labels)\n",
    "print(f\"Cohen's Kappa: {COHEN_KAPPA:.2f}\")\n",
    "\n",
    "# Kappa > 0.75 is a Strong agreement\n",
    "# Kappa = 0.4 - 0.75 is a Moderate agreement\n",
    "# Kappa < 0.4 is a Weak agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. BIAS DETECTION METRIC - Bland-Altman plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate systematic biases between VADER and BERT sentiment analysis\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# mean_scores = (np.array(individual_vader_sentiments) + np.array(individual_bert_sentiments)) / 2\n",
    "# diff_scores = np.array(individual_vader_sentiments) - np.array(individual_bert_sentiments)\n",
    "\n",
    "# # Create a scatter plot for the Bland-Altman analysis\n",
    "# sns.scatterplot(x=mean_scores, y=diff_scores)\n",
    "# plt.axhline(0, color='red', linestyle='dashed')  # No bias line\n",
    "# plt.xlabel(\"Mean Sentiment Score\")\n",
    "# plt.ylabel(\"VADER - BERT Sentiment Score Difference\")\n",
    "# plt.title(\"Bland-Altman Plot\")\n",
    "# plt.show()\n",
    "# # patterns indicate bias\n",
    "# # if most points are close to 0, the models mostly agree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRINSIC VALIDATION - CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import glob\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "\n",
    "# ground_truth = {}\n",
    "# path = os.path.join(\"GROUND_TRUTH_LABELS\", \"*.ndjson\")\n",
    "# ndjson_files_ground_truth = glob.glob(path)\n",
    "\n",
    "# # Load Ground Truth Labels\n",
    "# # ground_truth = {}  # Dictionary for easier comparison\n",
    "# # ndjson_files_ground_truth = glob.glob(\"/GROUND_TRUTH_LABELS/*.ndjson\")  # Adjust path as needed\n",
    "\n",
    "# for file in ndjson_files_ground_truth:\n",
    "#     with open(file, \"r\") as f:\n",
    "#         for line in f:\n",
    "#             entry = json.loads(line)\n",
    "#             ground_truth[entry[\"comment_id\"]] = entry[\"sentiment\"]\n",
    "\n",
    "# print(\"Sample Ground Truth Data:\", list(ground_truth.items())[:5])\n",
    "\n",
    "# # Convert VADER and BERT sentiment scores to labels (Positive, Neutral, Negative)\n",
    "# vader_labels = [to_sentiment_category(sentiment) for sentiment in individual_vader_sentiments]\n",
    "# bert_labels = [to_sentiment_category(sentiment) for sentiment in individual_bert_sentiments]\n",
    "\n",
    "# print(\"Sample VADER Predictions:\", vader_labels[:5])\n",
    "# print(\"Sample BERT Predictions:\", bert_labels[:5])\n",
    "\n",
    "# # Assuming the comment IDs are in the same order in ground_truth as in the sentiment lists\n",
    "# common_comment_ids = set(ground_truth.keys())  \n",
    "\n",
    "# y_true = [ground_truth[comment_id] for comment_id in common_comment_ids]\n",
    "# y_vader = vader_labels[:len(y_true)]  # Ensure same length \n",
    "# y_bert = bert_labels[:len(y_true)]  # Ensure same length \n",
    "\n",
    "# print(f\"Total Common Comments: {len(common_comment_ids)}\")\n",
    "\n",
    "# # Step 3.1: Compute and Plot Confusion Matrices\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# for ax, (y_pred, title) in zip(axes, [(y_vader, \"VADER\"), (y_bert, \"BERT\")]):\n",
    "#     cm = confusion_matrix(y_true, y_pred, labels=[\"Positive\", \"Neutral\", \"Negative\"])\n",
    "#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "#                 xticklabels=[\"Positive\", \"Neutral\", \"Negative\"],\n",
    "#                 yticklabels=[\"Positive\", \"Neutral\", \"Negative\"], ax=ax)\n",
    "#     ax.set_title(f\"Confusion Matrix: {title}\")\n",
    "#     ax.set_xlabel(\"Predicted Label\")\n",
    "#     ax.set_ylabel(\"True Label\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample VADER Sentiment Labels:\", vader_labels[:5])\n",
    "print(\"Sample BERT Sentiment Labels:\", bert_labels[:5])\n",
    "\n",
    "print(\"Sample Ground Truth Labels:\", list(ground_truth.values())[:5])\n",
    "\n",
    "print(\"Found NDJSON files:\", ndjson_files_ground_truth)\n",
    "\n",
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "print(os.listdir('GROUND_TRUTH_LABELS'))  # This should list the files in the folder\n",
    "\n",
    "import glob\n",
    "ndjson_files_ground_truth = glob.glob(\"GROUND_TRUTH_LABELS/*.ndjson\")  # or use absolute path here\n",
    "print(\"Found NDJSON files:\", ndjson_files_ground_truth)\n",
    "\n",
    "file_path = \"GROUND_TRUTH_LABELS/labeled_comments_xtqa50.ndjson\"  # Replace with actual filename\n",
    "try:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        print(f.readline())  # Try reading the first line\n",
    "except FileNotFoundError:\n",
    "    print(f\"File {file_path} not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load Ground Truth Labels\n",
    "ground_truth = {}\n",
    "path = os.path.join(\"GROUND_TRUTH_LABELS\", \"*.ndjson\")\n",
    "ndjson_files_ground_truth = glob.glob(path)\n",
    "\n",
    "for file in ndjson_files_ground_truth:\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            ground_truth[entry[\"comment_id\"]] = entry[\"sentiment\"]\n",
    "\n",
    "# Simulated VADER and BERT predictions (replace these with actual predictions)\n",
    "individual_vader_sentiments = {\"Negative\": -1, \"Neutral\": 0, \"Positive\": 1}\n",
    "individual_bert_sentiments = {\"Negative\": -1, \"Neutral\": 0, \"Positive\": 1}\n",
    "\n",
    "# Convert sentiment scores to labels\n",
    "vader_labels = {k: to_sentiment_category(v) for k, v in individual_vader_sentiments.items()}\n",
    "bert_labels = {k: to_sentiment_category(v) for k, v in individual_bert_sentiments.items()}\n",
    "\n",
    "# Find common comment IDs\n",
    "common_comment_ids = set(ground_truth.keys()) & set(vader_labels.keys()) & set(bert_labels.keys())\n",
    "\n",
    "y_true = [ground_truth[cid] for cid in common_comment_ids]\n",
    "y_vader = [vader_labels[cid] for cid in common_comment_ids]\n",
    "y_bert = [bert_labels[cid] for cid in common_comment_ids]\n",
    "\n",
    "# Generate Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "for ax, (y_pred, title) in zip(axes, [(y_vader, \"VADER\"), (y_bert, \"BERT\")]):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[\"Positive\", \"Neutral\", \"Negative\"])\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"Positive\", \"Neutral\", \"Negative\"],\n",
    "                yticklabels=[\"Positive\", \"Neutral\", \"Negative\"], ax=ax)\n",
    "    ax.set_title(f\"Confusion Matrix: {title}\")\n",
    "    ax.set_xlabel(\"Predicted Label\")\n",
    "    ax.set_ylabel(\"True Label\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attempt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Function to convert sentiment scores to labels\n",
    "def to_sentiment_category(score):\n",
    "    if score >= 0.05:\n",
    "        return \"Positive\"\n",
    "    elif score <= -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Load Ground Truth Labels\n",
    "ground_truth = {}\n",
    "path = os.path.join(\"GROUND_TRUTH_LABELS\", \"*.ndjson\")\n",
    "ndjson_files_ground_truth = glob.glob(path)\n",
    "\n",
    "for file in ndjson_files_ground_truth:\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            ground_truth[entry[\"comment_id\"]] = entry[\"sentiment\"]\n",
    "\n",
    "validation_labels_dir = config.ROOT_DIR / 'validation_labels' / 'steward_decision_submissions'\n",
    "labeled_comments = tuple(\n",
    "    labeled_comment\n",
    "    for file in validation_labels_dir.glob('*.ndjson')\n",
    "    for labeled_comment in stream_ndjson(file)\n",
    ")\n",
    "\n",
    "# Simulated VADER and BERT predictions (Replace with actual predictions)\n",
    "# These should be dictionaries where keys are comment IDs and values are sentiment scores.\n",
    "\n",
    "\n",
    "# # Convert sentiment scores to categorical labels\n",
    "# vader_labels = {k: to_sentiment_category(v) for k, v in individual_vader_sentiments.items()}\n",
    "# bert_labels = {k: to_sentiment_category(v) for k, v in individual_bert_sentiments.items()}\n",
    "\n",
    "# # Find common comment IDs\n",
    "# common_comment_ids = set(ground_truth.keys()) & set(vader_labels.keys()) & set(bert_labels.keys())\n",
    "\n",
    "# if not common_comment_ids:\n",
    "#     print(\"No common comment IDs found. Check your data sources.\")\n",
    "# else:\n",
    "\n",
    "\n",
    "# display(comments_df.loc[comments_df['id'] == 'iqr5d98', 'body'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_true = [labeled_comment['sentiment'] for labeled_comment in labeled_comments]\n",
    "y_vader = [to_sentiment_category(vader_analyzer.polarity_scores(comments_df.loc[comments_df['id'] == labeled_comment['comment_id'], 'body'].iloc[0])['compound']) for labeled_comment in labeled_comments]\n",
    "y_bert = [to_sentiment_category(bert_sentiment(comments_df.loc[comments_df['id'] == labeled_comment['comment_id'], 'body'].iloc[0])) for labeled_comment in labeled_comments]\n",
    "\n",
    "# Generate Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for ax, (y_pred, title) in zip(axes, [(y_vader, \"VADER\"), (y_bert, \"BERT\")]):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[\"Positive\", \"Neutral\", \"Negative\"])\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"Positive\", \"Neutral\", \"Negative\"],\n",
    "                yticklabels=[\"Positive\", \"Neutral\", \"Negative\"], ax=ax)\n",
    "    ax.set_title(f\"Confusion Matrix: {title}\")\n",
    "    ax.set_xlabel(\"Predicted Label\")\n",
    "    ax.set_ylabel(\"True Label\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attempt 4: accuracy, precision, recall and F1 validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Compute overall accuracy\n",
    "accuracy = accuracy_score(y_true, y_vader)\n",
    "print(f\"VADER Accuracy: {accuracy:.4f}\")\n",
    "accuracy = accuracy_score(y_true, y_bert)\n",
    "print(f\"BERT Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compute precision, recall, and F1-score for each class\n",
    "labels = [\"Positive\", \"Neutral\", \"Negative\"]\n",
    "for label in labels:\n",
    "    vader_precision = precision_score(y_true, y_vader, labels=[label], average=\"macro\", zero_division=0)\n",
    "    vader_recall = recall_score(y_true, y_vader, labels=[label], average=\"macro\", zero_division=0)\n",
    "    vader_f1 = f1_score(y_true, y_vader, labels=[label], average=\"macro\", zero_division=0)\n",
    "\n",
    "    bert_precision = precision_score(y_true, y_bert, labels=[label], average=\"macro\", zero_division=0)\n",
    "    bert_recall = recall_score(y_true, y_bert, labels=[label], average=\"macro\", zero_division=0)\n",
    "    bert_f1 = f1_score(y_true, y_bert, labels=[label], average=\"macro\", zero_division=0)\n",
    "\n",
    "    print(f\"\\nMetrics for class '{label}':\")\n",
    "    print(f\"  VADER -> Precision: {vader_precision:.4f}, Recall: {vader_recall:.4f}, F1-score: {vader_f1:.4f}\")\n",
    "    print(f\"  BERT  -> Precision: {bert_precision:.4f}, Recall: {bert_recall:.4f}, F1-score: {bert_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
