{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Generator, Callable\n",
    "from pathlib import Path\n",
    "import typing\n",
    "from typing import Any, TypeAlias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import re\n",
    "from functools import partial, reduce\n",
    "from tqdm import tqdm\n",
    "from IPython.display import (\n",
    "    display, # type: ignore[reportUnknownVariableType]\n",
    "    Markdown,\n",
    ")\n",
    "\n",
    "import importlib\n",
    "\n",
    "from config.fastf1 import fastf1\n",
    "from config import config\n",
    "import src.data.constants as dataset_constants\n",
    "importlib.reload(dataset_constants);\n",
    "import src.data.loader\n",
    "importlib.reload(src.data.loader);\n",
    "from src.data.loader import stream_ndjson, load_submissions_df, load_comments_df\n",
    "from src.data.preprocessing import concatenate_submissions_and_comments\n",
    "\n",
    "from src.utils import (\n",
    "    temporary_pandas_options,\n",
    "    display_full_dataframe,\n",
    "    hide_index,\n",
    "    compose,\n",
    ")\n",
    "from src import utils\n",
    "utils.set_random_seeds()\n",
    "DEVICE = utils.get_device()\n",
    "\n",
    "import logging\n",
    "logging.getLogger('fastf1').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_ndjson_streamer = partial(stream_ndjson, limit=10)\n",
    "f1_submissions_df = load_submissions_df(dataset_constants.RawFile.FORMULA1_SUBMISSIONS, f1_ndjson_streamer)\n",
    "f1_comments_df = load_comments_df(dataset_constants.RawFile.FORMULA1_COMMENTS, f1_ndjson_streamer)\n",
    "\n",
    "f1_submissions_df = load_submissions_df(dataset_constants.RawFile.FORMULA1POINT5_SUBMISSIONS)\n",
    "f1_comments_df = load_comments_df(dataset_constants.RawFile.FORMULA1POINT5_COMMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(Markdown('### r/formula1 submissions:'), f1_submissions_df.head(n))\n",
    "    display(Markdown('### r/formula1 comments:'), f1_comments_df.head(n))\n",
    "    display(Markdown('### r/formula1point5 submissions:'), f1_submissions_df.head(n))\n",
    "    display(Markdown('### r/formula1point5 comments:'), f1_comments_df.head(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df = concatenate_submissions_and_comments(f1_submissions_df, f1_comments_df)\n",
    "f15_df = concatenate_submissions_and_comments(f1_submissions_df, f1_comments_df)\n",
    "\n",
    "n = 8\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(Markdown('### r/formula1 posts:'), f1_df.head(n))\n",
    "    display(Markdown('### r/formula1point5 posts:'), f15_df.head(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Steward_keywords = {\n",
    "    'penalty', 'steward', 'decision', 'appeal', 'review', 'ruling', 'investigation', 'regulation',\n",
    "    'seconds', 'sec', \n",
    "    'collision', 'crash', 'incident', 'overtake', 'virtual safety car', 'blocking', 'brake test', 'contact',\n",
    "    'red flag', 'yellow flag', \n",
    "    'controversial', 'rigged', 'corrupt', 'bias', 'protest', 'FIA'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_submissions_df = load_submissions_df(\n",
    "    dataset_constants.RawFile.FORMULA1_SUBMISSIONS,\n",
    "    columns=dataset_constants.DEFAULT_SUBMISSION_COLUMNS | {'permalink', 'post_hint'},\n",
    ")                                  \n",
    "\n",
    "f1_comments_df = load_comments_df(\n",
    "    dataset_constants.RawFile.FORMULA1_COMMENTS,\n",
    "    columns=dataset_constants.DEFAULT_COMMENT_COLUMNS | {'link_id'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to retrieve relevant posts, related to news, technical and discussion\n",
    "\n",
    "relevant_tags = ['post-news', 'post-discussion', 'post-technical']   \n",
    "filtered_f1_submissions_df = f1_submissions_df[f1_submissions_df['link_flair_richtext'].isin(relevant_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_permalink = '/r/formula1/comments/yfcxll/fia_document_of_stewards_decision_for_alpines/'\n",
    "sample_submission = f1_submissions_df[f1_submissions_df['permalink'] == sample_submission_permalink].iloc[0]\n",
    "sample_submission_link_id = f't3_{sample_submission['id']}'\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(sample_submission)\n",
    "\n",
    "comments_in_sample_submission = f1_comments_df[f1_comments_df['link_id'] == sample_submission_link_id]\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(comments_in_sample_submission)\n",
    "\n",
    "display(comments_in_sample_submission['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VADER SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# compound is a number indicating the positiveness, negativeness of a document, normalizing between -1 and 1.\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "comments_in_sample_submission['compound'] = comments_in_sample_submission['body'].apply(\n",
    "    lambda text: analyzer.polarity_scores(text)['compound']\n",
    ")   #creates a data series with a score (float) given an input comment\n",
    "\n",
    "NOEMER = np.abs(comments_in_sample_submission['score']).sum()\n",
    "average_sentiment = (comments_in_sample_submission['compound'] * comments_in_sample_submission['score']).sum() / NOEMER    \n",
    "    \n",
    "print(average_sentiment)\n",
    "\n",
    "def Vader_sentiment(average_sentiment):\n",
    "    if average_sentiment >= 0.05:\n",
    "        return \"Positive\"\n",
    "    elif average_sentiment <= -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "print(Vader_sentiment(average_sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model.to(DEVICE);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up the path to your dataset\n",
    "data = Path(\"data/raw/formula1_comments.ndjson\")\n",
    "\n",
    "# Limit the number of records to process (e.g., 1000)\n",
    "n = 1000\n",
    "limited_data = []\n",
    "\n",
    "# Open the file and read only the first 'n' lines\n",
    "with data.open('r', encoding='utf-8') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i >= n:\n",
    "            break  # Stop after reading 'n' lines\n",
    "        limited_data.append(json.loads(line))  # Parse each line as JSON\n",
    "\n",
    "# Create a DataFrame with the limited data\n",
    "df = pd.DataFrame(limited_data)\n",
    "\n",
    "# Print the columns of the DataFrame to understand the structure\n",
    "print(\"Columns in DataFrame:\", df.columns)\n",
    "\n",
    "# Assuming dataset_constants.SUBMISSION_COLUMN_DTYPES is defined correctly\n",
    "# Example of dataset_constants (adjust based on your DataFrame columns)\n",
    "dataset_constants.SUBMISSION_COLUMN_DTYPES = {\n",
    "    'author': 'str',\n",
    "    'created_utc': 'float64',\n",
    "    'body': 'str',  # Adjust to your actual column name (e.g., 'body' instead of 'selftext')\n",
    "    # Add other columns and types here as needed\n",
    "}\n",
    "\n",
    "# Apply dtype conversion to existing columns\n",
    "df = df.astype(dataset_constants.SUBMISSION_COLUMN_DTYPES)\n",
    "\n",
    "# Display the DataFrame with additional details\n",
    "with display_full_dataframe():\n",
    "    display(df)\n",
    "    display(type(df['preview'].iloc[0]))\n",
    "    display(type(df.dtypes['author_flair_background_color']))\n",
    "    display(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some old code below, better not to throw away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# # data = config.DATA_DIR / 'raw'  'formula1_submissions'\n",
    "# data = Path(\"data/raw/formula1_comments.ndjson\")\n",
    "# with data.open('r') as file:\n",
    "#     df = pd.DataFrame((json.load(file),)).astype(dataset_constants.SUBMISSION_COLUMN_DTYPES)\n",
    "\n",
    "# with display_full_dataframe():\n",
    "#     display(df)\n",
    "#     display(type(df['preview'].iloc[0]))\n",
    "#     display(type(df.dtypes['author_flair_background_color']))\n",
    "#     display(df.dtypes)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steward_decision_related_words = {\n",
    "    'penalty', 'steward', 'decision', 'appeal', 'review', 'ruling', 'investigation', 'regulation',\n",
    "    'seconds', 'sec', \n",
    "    'collision', 'crash', 'incident', 'overtake', 'virtual safety car', 'blocking', 'brake test', 'contact',\n",
    "    'red flag', 'yellow flag', \n",
    "    'controversial', 'rigged', 'corrupt', 'bias', 'protest', 'FIA', 'document'}\n",
    "\n",
    "words_regex = ''.join(fr'\\b{word}\\b|' for word in steward_decision_related_words)[:-1]\n",
    "steward_decision_pattern = re.compile(words_regex, flags=re.IGNORECASE)\n",
    "\n",
    "# steward_decision_pattern = re.compile(r'\\bdocument\\b', flags=re.IGNORECASE)\n",
    "steward_decision_submissions_df = f1_submissions_df[\n",
    "    f1_submissions_df['title'].apply(lambda title: steward_decision_pattern.search(title) is not None)].copy()\n",
    "\n",
    "with display_full_dataframe():\n",
    "    print(len(steward_decision_submissions_df))\n",
    "    display(steward_decision_submissions_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for index, steward_decision_submission in steward_decision_submissions_df.iterrows():\n",
    "    submission_link_id = f't3_{steward_decision_submission['id']}'\n",
    "    comments_df = f1_comments_df[f1_comments_df['link_id'] == submission_link_id].copy()\n",
    "\n",
    "    # display(comments_df)\n",
    "    # print(comments_df.empty)\n",
    "    # x = comments_df.empty\n",
    "    # break\n",
    "\n",
    "    if comments_df.empty:\n",
    "        steward_decision_submissions_df.loc[index, 'average_sentiment_vader'] = np.nan\n",
    "        continue\n",
    "    \n",
    "    NOEMER = np.abs(comments_df['score']).sum()\n",
    "    \n",
    "    if NOEMER == 0:\n",
    "        steward_decision_submissions_df.loc[index, 'average_sentiment_vader'] = np.nan\n",
    "        continue\n",
    "\n",
    "\n",
    "    comments_df.loc[:, 'compound'] = comments_df['body'].apply(\n",
    "        lambda text: analyzer.polarity_scores(text)['compound']\n",
    "    )   #creates a data series with a score (float) given an input comment\n",
    "\n",
    "    average_sentiment = (comments_df['compound'] * comments_df['score']).sum() / NOEMER\n",
    "    steward_decision_submissions_df.loc[index, 'average_sentiment_vader'] = average_sentiment\n",
    "\n",
    "    # score = 0\n",
    "    # compound_list = []\n",
    "    # for _, comment in comments_df.iterrows():\n",
    "    #     compound_list.append(analyzer.polarity_scores(comment['body'])['compound'])\n",
    "    #     comment_score = comments_df['score']\n",
    "    #     score += np.abs(comment_score)\n",
    "\n",
    "    # compound_sum = np.array(compound_list).sum()\n",
    "    # x = comments_df['compound'] * comments_df['score'] / compound_sum\n",
    "    # print(Vader_sentiment(average_sentiment))\n",
    "    # print(average_sentiment)\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(steward_decision_submissions_df)\n",
    "    display(steward_decision_submissions_df['average_sentiment_vader'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "def BERT_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    sentiment_score = torch.argmax(logits, dim=1).item()  # 0: negative, 1: neutral, 2: positive\n",
    "    return (sentiment_score - 1)\n",
    "\n",
    "for index, steward_decision_submission in steward_decision_submissions_df.iterrows():\n",
    "    submission_link_id = f't3_{steward_decision_submission['id']}'\n",
    "    comments_df = f1_comments_df[f1_comments_df['link_id'] == submission_link_id].copy()\n",
    "\n",
    "    if comments_df.empty:\n",
    "        steward_decision_submissions_df.loc[index, 'average_sentiment_bert'] = np.nan\n",
    "        continue\n",
    "    \n",
    "    NOEMER = np.abs(comments_df['score']).sum()\n",
    "    \n",
    "    if NOEMER == 0:\n",
    "        steward_decision_submissions_df.loc[index, 'average_sentiment_bert'] = np.nan\n",
    "        continue\n",
    "\n",
    "    comments_df.loc[:, 'compound'] = comments_df['body'].apply(BERT_sentiment)\n",
    "        # lambda text: analyzer.polarity_scores(text)['compound']\n",
    "        # bert induced sentiment\n",
    "\n",
    "    average_sentiment = (comments_df['compound'] * comments_df['score']).sum() / NOEMER\n",
    "    steward_decision_submissions_df.loc[index, 'average_sentiment_bert'] = average_sentiment\n",
    "\n",
    "# with display_full_dataframe():\n",
    "    # display(steward_decision_submissions_df)\n",
    "    # display(steward_decision_submissions_df['average_sentiment_bert'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.abs(steward_decision_submissions_df['average_sentiment_bert'] - steward_decision_submissions_df['average_sentiment_vader']).sum() \\\n",
    "    / len(steward_decision_submissions_df.index)\n",
    ")\n",
    "\n",
    "with display_full_dataframe():\n",
    "    display(steward_decision_submissions_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
