{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "# from transformers import TFAutoModelForSequenceClassification\n",
    "# from transformers import AutoTokenizer, AutoConfig\n",
    "# import numpy as np\n",
    "# from scipy.special import softmax\n",
    "\n",
    "# text = \"Max verstappen is going to win\"\n",
    "# MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "# sentiment_task = pipeline(\"sentiment-analysis\", model=MODEL, tokenizer=MODEL)\n",
    "# sentiment_task(text)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "# model_config = AutoConfig.from_pretrained(MODEL)\n",
    "# # PT\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(config.MODELS_DIR / 'sentiment_model.pt')\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)\n",
    "# scores = output[0][0].detach().numpy()\n",
    "# scores = softmax(scores)\n",
    "# # # TF\n",
    "# # model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# # model.save_pretrained(MODEL)\n",
    "# # text = \"Covid cases are increasing fast!\"\n",
    "# # encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# # output = model(encoded_input)\n",
    "# # scores = output[0][0].numpy()\n",
    "# # scores = softmax(scores)\n",
    "# # Print labels and scores\n",
    "# print(scores)\n",
    "# ranking = np.argsort(scores)\n",
    "# ranking = ranking[::-1]\n",
    "# print(ranking)\n",
    "# for i in range(scores.shape[0]):\n",
    "#     l = model_config.id2label[ranking[i]]\n",
    "#     s = scores[ranking[i]]\n",
    "#     print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing.correct_spelling_in_text_spacy('Mx Verstappening and Charls Lecerc are, just like this, a very good (/bad) example of drivers... and, voilÃ !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver_pattern = re.compile(r'Verstappen|Hamilton|Leclerc')\n",
    "\n",
    "# filtered_f1_df = f1_df[f1_df[\"text\"].str.contains(driver_pattern)]\n",
    "\n",
    "# with display_full_dataframe():\n",
    "#     display(filtered_f1_df.head(3))\n",
    "    \n",
    "# filtered_f1_df['text'] = filtered_f1_df['text'].apply(preprocessing.correct_spelling_in_text_spacy)\n",
    "\n",
    "# with display_full_dataframe():\n",
    "#     display(filtered_f1_df.head(3))\n",
    "#     print(len(filtered_f1_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver_scores = {\"Verstappen\": [], \"Hamilton\": [], \"Leclerc\": []}\n",
    "\n",
    "# tokens = text.split()\n",
    "# corrected_tokens = [preprocessing.correct_spelling_symspell(word) for word in tokens]\n",
    "# preprocessed_text = \" \".join(combined_tokens)\n",
    "\n",
    "# for text in tqdm(filtered_f1_df[\"text\"]):\n",
    "#     # print(len(text))\n",
    "#     if len(text) > 1493:\n",
    "#         continue\n",
    "#     sentiment_result = sentiment_task(text)\n",
    "#     # print(sentiment_result)\n",
    "#     for driver in driver_scores.keys():\n",
    "#         if driver in text:\n",
    "#             driver_scores[driver].append(sentiment_result[0]['score'])\n",
    "\n",
    "# # Aggregate scores\n",
    "# final_scores = {driver: np.mean(scores) for driver, scores in driver_scores.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
